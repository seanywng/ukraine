---
title: "Untitled"
output: html_document
date: '2023-01-02'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rvest)
library(tidyverse)
library(lubridate)
library(magrittr)

theme_set(theme_light())
```

## Just be aware, the data was scraped on 04/01/2022 or just find the minmax on the dates 

```{r}

# Read in the website
base_url <- "https://reliefweb.int/updates?advanced-search=%28PC241%29"
base_webpage <- read_html(base_url)

html_attr(html_nodes(base_webpage, "a"), "href")  

new_urls <- "https://reliefweb.int/report/ukraine/%s"

links <- paste0("https://reliefweb.int/updates?advanced-search=%28PC241%29_%28F10%29&page=", 0:591)

result <- lapply(links, function(x) x %>% 
                   read_html %>% 
                   html_nodes("a") %>% 
                   html_attr("href"))

```

## Scrape_links()

```{r}
scrape_links <-  function(page){ 
  page %>% 
    read_html() %>% 
    html_nodes(".rw-river-article__title a") %>% 
    html_attr("href") %>% 
    as_tibble()
}



links <- paste0("https://reliefweb.int/updates?advanced-search=%28PC241%29_%28DA20220101-20230104%29&page=", 0:194)

results <- bind_rows(map(links, possibly(scrape_links, NA)))

results %>% 
  write_csv("./data/reliefweb_ukr_links.csv")

```


```{r}
ukr_links <- read_csv("./data/reliefweb_ukr_links.csv") %>% 
  rename(url = value) %>% 
  pull(url)

small <- read_csv("./data/reliefweb_ukr_links.csv") %>% 
  rename(url = value) %>% 
  sample_n(10) %>% 
  pull(url)

medium <- ukr_links[1:500]

head(medium)
head(ukr_links)

a <- ukr_links[1:400]
b <- ukr_links[401:800]
c <- ukr_links[801:1200]
d <- ukr_links[1201:1600] 
e <- ukr_links[1601:2000]
f <- ukr_links[2001:2400]
g <- ukr_links[2401:2800]
h <- ukr_links[2801:3200]
i <- ukr_links[3201:3600]
j <- ukr_links[3601:3895]

j <- read_csv("./data/reliefweb_ukr_links.csv") %>% 
  rename(url = value) %>% 
  filter(url != "https://reliefweb.int/report/ukraine/joint-un-mission-visits-humanitarian-development-nexus-projects-eastern-ukraine-0") %>% 
  pull(url)

j <- j[3601:3895]


```


## Scrape_page()


```{r}



scrape_page <- function(link) {
  
  page <- read_html(link)
  
  scraped <- tibble()
  
  title <- page %>% 
    html_nodes(".rw-page-title") %>% 
    html_text2()
  type <- page %>% 
    html_nodes(".rw-article__header--with-meta .rw-entity-meta__tag-value--simple") %>% 
    html_text2()
  body <- page %>% 
    html_nodes(".rw-report__content p") %>% 
    html_text2() %>% 
    toString() 
  source <-  page %>% 
    html_nodes(".rw-article__header--with-meta .rw-entity-meta__tag-link") %>% 
    html_text2()
  date <-  page %>%
    html_nodes(".rw-article__header--with-meta .rw-entity-meta__tag-value--published time") %>%
    html_text2() 
  theme <- page %>% 
    html_nodes(".rw-entity-meta__tag-value--theme") %>% 
    html_text2() 
  link <- link
  
  rbind(scraped,
        tibble(title, type, source, theme, date, body, link))

  
}

```


```{r}

## Get Ready ## 

j_scraped <- j %>% 
  map_df(., possibly(scrape_page, character(0), quiet = FALSE))

j_scraped <- j[1:294] %>% map_df(., possibly(scrape_page, character(0), quiet = FALSE))

## This is the core function ## 
map_df(., possibly(scrape_page, character(0), quiet = FALSE))

# This one works; find the other rows that work 
# So, essentially, there has to be a better way to find broken links, but as of the moment, 
# breaking all the pages to be scraped up  into pieces, seems to be our current best option 
d_scraped <- d[-256] %>% map_df(., possibly(scrape_page, character(0), quiet = FALSE))

# This one threw up an error 
d[256] %>% map_df(., possibly(scrape_page, character(0), quiet = FALSE))

# This is the kind of crap you were doing once you had narrowed down which tranche had the faulty link 
d2 <- d[257:400] %>% map_df(., possibly(scrape_page, character(0), quiet = FALSE))

# Get rid of this: "https://reliefweb.int/report/ukraine/joint-un-mission-visits-humanitarian-development-nexus-projects-eastern-ukraine-0"
  
# Get rid of this too: "https://reliefweb.int/report/ukraine/human-rights-council-hears-21-dignitaries-it-continues-high-level-segment-speakers"

# D not working, but d[1:200] does work 

```


```{r}

a_scraped %>% write_csv("./data/a_scraped.csv")
b_scraped %>% write_csv("./data/b_scraped.csv")
c_scraped %>% write_csv("./data/c_scraped.csv")
d_scraped %>% write_csv("./data/d_scraped.csv")
e_scraped %>% write_csv("./data/e_scraped.csv")
f_scraped %>% write_csv("./data/f_scraped.csv")
g_scraped %>% write_csv("./data/g_scraped.csv")
h_scraped %>% write_csv("./data/h_scraped.csv")
i_scraped %>% write_csv("./data/i_scraped.csv")
j_scraped %>% write_csv("./data/j_scraped.csv")

rbind(a_scraped, 
      b_scraped, 
      c_scraped, 
      d_scraped, 
      e_scraped, 
      f_scraped, 
      g_scraped, 
      h_scraped, 
      i_scraped, 
      j_scraped) %>% 
  write_csv("scraped_full_20230101_20220101.csv")
```


```{r}
page %>%
  html_nodes(".rw-report__content p") %>%
  html_text2() %>% 
  toString() %>% 
    as_tibble() 
```


### Working script
These are the components of the core scraping function 
Assembled individually here to ensure that each functions
I must say, however, that I need to pay more attention to the selections from the SelectorGadget. 

```{r}

ukr_scraped <- tibble()

url <- "https://reliefweb.int/report/ukraine/war-ukraine-situation-report-who-ukraine-country-office-issue-no38-30-december-2022"

page <- read_html(url)

title <- page %>% html_nodes(".rw-page-title") %>% html_text2()
type <- page %>% html_nodes(".rw-article__header--with-meta .rw-entity-meta__tag-value--simple") %>% html_text2()
body <- page %>% html_nodes(".rw-report__content p") %>% html_text2() %>% as_tibble()
source <- page %>% html_nodes(".rw-article__header--with-meta .rw-entity-meta__tag-link") %>% html_text2()
date <- page %>% html_nodes(".rw-article__header--with-meta .rw-entity-meta__tag-value--published time") %>% html_text2()
theme <- page %>% html_nodes(".rw-entity-meta__tag-value--theme") %>% html_text2()

ukr_scraped <- tibble(title, type, source, theme, date, body)

```

## A fix for your stupidity

So, let us first scrape the addresses of the french and spanish articles so that they may be removed frm the dataset. 

We're going to make use of scrape_links() again 

```{r}
spanish_links <- paste0("https://reliefweb.int/updates?advanced-search=%28PC241%29_%28L269%29_%28DA20220101-20230104%29&page=", 0:1)

french_links <- paste0("https://reliefweb.int/updates?advanced-search=%28PC241%29_%28L268%29_%28DA20220101-20230104%29&page=", 0:1)

rbind(bind_rows(map(spanish_links, possibly(scrape_links, NA))),
      bind_rows(map(french_links, possibly(scrape_links, NA)))) %>% 
  write_csv("./data/links_to_remove.csv")

```


```{r}
# The language tag isn't really necessary now, just bear this in mind for the future
language <- page %>% html_nodes(".rw-entity-meta__tag-value--last .rw-entity-meta__tag-link") %>% 
  html_text2()
```

