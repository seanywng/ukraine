---
title: "Untitled"
output: html_document
date: '2023-01-02'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rvest)
library(tidyverse)
library(lubridate)
library(magrittr)
library(httr)
library(jsonlite)

`%out%` <- Negate(`%in%`)
theme_set(theme_light())
```

## Just be aware, the data was scraped on 04/01/2022 or just find the minmax on the dates 

```{r}

# Read in the website
base_url <- "https://reliefweb.int/updates?advanced-search=%28PC241%29"
base_webpage <- read_html(base_url)

html_attr(html_nodes(base_webpage, "a"), "href")  

new_urls <- "https://reliefweb.int/report/ukraine/%s"

links <- paste0("https://reliefweb.int/updates?advanced-search=%28PC241%29_%28F10%29&page=", 0:591)

result <- lapply(links, function(x) x %>% 
                   read_html %>% 
                   html_nodes("a") %>% 
                   html_attr("href"))

```

## Scrape_links()

```{r}
scrape_links <-  function(page){ 
  
  Sys.sleep(1)
  
  page %>% 
    read_html() %>% 
    html_nodes(".rw-river-article__title a") %>% 
    html_attr("href") %>% 
    as_tibble()
}


```

```{r}
ukr_urls <- paste0("https://reliefweb.int/updates?advanced-search=%28PC241%29_%28DA20220101-20230104%29&page=", 0:194)

ukr_results <- bind_rows(map(ukr_urls, possibly(scrape_links, NA)))

ukr_results %>% 
  write_csv("./data/reliefweb_ukr_links.csv")
```


```{r}
ukr_links <- read_csv("./data/reliefweb_ukr_links.csv") %>% 
  rename(url = value) %>% 
  pull(url)

small <- read_csv("./data/reliefweb_ukr_links.csv") %>% 
  rename(url = value) %>% 
  sample_n(10) %>% 
  pull(url)

medium <- ukr_links[1:500]

head(medium)
head(ukr_links)

a <- ukr_links[1:400]
b <- ukr_links[401:800]
c <- ukr_links[801:1200]
d <- ukr_links[1201:1600] 
e <- ukr_links[1601:2000]
f <- ukr_links[2001:2400]
g <- ukr_links[2401:2800]
h <- ukr_links[2801:3200]
i <- ukr_links[3201:3600]
j <- ukr_links[3601:3895]

j <- read_csv("./data/reliefweb_ukr_links.csv") %>% 
  rename(url = value) %>% 
  filter(url != "https://reliefweb.int/report/ukraine/joint-un-mission-visits-humanitarian-development-nexus-projects-eastern-ukraine-0") %>% 
  pull(url)

j <- j[3601:3895]


```


## Scrape_page()


```{r}

scrape_page <- function(link) {
  
  Sys.sleep(1)
  
  page <- read_html(link)
  
  scraped <- tibble()
  
  title <- page %>% 
    html_nodes(".rw-page-title") %>% 
    html_text2()
  type <- page %>% 
    html_nodes(".rw-article__header--with-meta .rw-entity-meta__tag-value--simple") %>% 
    html_text2()
  body <- page %>% 
    html_nodes(".rw-report__content p") %>% 
    html_text2() %>% 
    toString() 
  source <-  page %>% 
    html_nodes(".rw-article__header--with-meta .rw-entity-meta__tag-link") %>% 
    html_text2()
  date <-  page %>%
    html_nodes(".rw-article__header--with-meta .rw-entity-meta__tag-value--published time") %>%
    html_text2() 
  theme <- page %>% 
    html_nodes(".rw-entity-meta__tag-value--theme") %>% 
    html_text2() 
  link <- link
  
  rbind(scraped,
        tibble(title, type, source, theme, date, body, link))
  

  
}

```


```{r}

## Get Ready ## 

j_scraped <- j %>% 
  map_df(., possibly(scrape_page, character(0), quiet = FALSE))

j_scraped <- j[1:294] %>% map_df(., possibly(scrape_page, character(0), quiet = FALSE))

## This is the core function ## 
map_df(., possibly(scrape_page, character(0), quiet = FALSE))

# This one works; find the other rows that work 
# So, essentially, there has to be a better way to find broken links, but as of the moment, 
# breaking all the pages to be scraped up  into pieces, seems to be our current best option 
d_scraped <- d[-256] %>% map_df(., possibly(scrape_page, character(0), quiet = FALSE))

# This one threw up an error 
d[256] %>% map_df(., possibly(scrape_page, character(0), quiet = FALSE))

# This is the kind of crap you were doing once you had narrowed down which tranche had the faulty link 
d2 <- d[257:400] %>% map_df(., possibly(scrape_page, character(0), quiet = FALSE))

# Get rid of this: "https://reliefweb.int/report/ukraine/joint-un-mission-visits-humanitarian-development-nexus-projects-eastern-ukraine-0"
  
# Get rid of this too: "https://reliefweb.int/report/ukraine/human-rights-council-hears-21-dignitaries-it-continues-high-level-segment-speakers"

# D not working, but d[1:200] does work 

```


```{r}

a_scraped %>% write_csv("./data/a_scraped.csv")
b_scraped %>% write_csv("./data/b_scraped.csv")
c_scraped %>% write_csv("./data/c_scraped.csv")
d_scraped %>% write_csv("./data/d_scraped.csv")
e_scraped %>% write_csv("./data/e_scraped.csv")
f_scraped %>% write_csv("./data/f_scraped.csv")
g_scraped %>% write_csv("./data/g_scraped.csv")
h_scraped %>% write_csv("./data/h_scraped.csv")
i_scraped %>% write_csv("./data/i_scraped.csv")
j_scraped %>% write_csv("./data/j_scraped.csv")

rbind(a_scraped, 
      b_scraped, 
      c_scraped, 
      d_scraped, 
      e_scraped, 
      f_scraped, 
      g_scraped, 
      h_scraped, 
      i_scraped, 
      j_scraped) %>% 
  write_csv("scraped_full_20230101_20220101.csv")
```


```{r}
page %>%
  html_nodes(".rw-report__content p") %>%
  html_text2() %>% 
  toString() %>% 
    as_tibble() 
```

### Scrape Myanmar and Yemen 

#### Myanmar

```{r}
mmr_urls <- paste0("https://reliefweb.int/updates?advanced-search=%28PC165%29_%28L267%29_%28DO20210101-20230118%29&page=", 0:97)

mmr_urls1 <- paste0("https://reliefweb.int/updates?advanced-search=%28PC165%29_%28L267%29_%28DO20210101-20230118%29&page=", 0:50)

mmr_urls2 <- paste0("https://reliefweb.int/updates?advanced-search=%28PC165%29_%28L267%29_%28DO20210101-20230118%29&page=", 51:97)

mmr_results1 <- map_df(mmr_urls1, possibly(scrape_links, NA)) %>% 
  rbind()

mmr_results2 <- map_df(mmr_urls2, possibly(scrape_links, NA)) %>% rbind()

mmr_results <- rbind(mmr_results1, mmr_results2)

mmr_results %>% write_csv("./data/mmr_links.csv")


```

```{r}
mmr_links_2022 <- mmr_links

mmr_links_2021 <- mmr_links %>% 
  filter(url %out% mmr_links_2022) %>% pull(url)

d <- mmr_links_2021[0:300]
e <- mmr_links_2021[301:600]
f <- mmr_links_2021[601:900]
f <- f[-163]
g <- mmr_links_2021[900:1030]
```

```{r}
g_scraped <- g %>% 
  map_df(., possibly(scrape_page, character(0), quiet = FALSE)) %>% rbind()
```

```{r}
a <- mmr_links[1:300] 
a <- a[-233]
b <- mmr_links[301:600]
b <- b[-92]
c <- mmr_links[601:922]

```

```{r}
# These are the ones that were removed 
# We can check to see if the links are broken 
a[233]
b[92]
f[163]
```

https://reliefweb.int/map/myanmar/myanmar-emergency-overview-map-number-people-displaced-feb-2021-and-remain-displaced-26-sep-2022
https://reliefweb.int/report/myanmar/cccm-camp-profile-i-ohn-taw-gyi-north-camp-i-sittwe-i-rakhine-state-q2-apr-jun-2022
https://reliefweb.int/map/myanmar/burma-bangladesh-usg-response-regional-crisis-042321


```{r}
a_scraped %>% write_csv("./data/a_scraped.csv")
b_scraped %>% write_csv("./data/b_scraped.csv")
c_scraped %>% write_csv("./data/c_scraped.csv")
d_scraped %>% write_csv("./data/d_scraped.csv")
e_scraped %>% write_csv("./data/e_scraped.csv")
f_scraped %>% write_csv("./data/f_scraped.csv")
g_scraped %>% write_csv("./data/g_scraped.csv")


rbind(a_scraped, 
      b_scraped, 
      c_scraped, 
      d_scraped, 
      e_scraped, 
      f_scraped, 
      g_scraped) %>% 
  write_csv("C:/Users/seanywng/Documents/R/reliefweb_web_scraping/data/mmr_scraped.csv")

```





```{r}
mmr_links <- 
  read_csv("C:/Users/seanywng/Documents/R/reliefweb_web_scraping/data/mmr_links.csv") %>% 
  rename(url = value) 

mmr_scraped <- mmr_links %>% 
  map_df(., possibly(scrape_page, character(0), quiet = FALSE))

mmr_scraped %>% 
  write_csv("./data/_mmr_scraped_full_20230101_20220101.csv")
```


#### Yemen 

```{r}
yem_urls <- paste0("https://reliefweb.int/updates?advanced-search=%28PC255%29_%28L267%29_%28DO20220101-20230101%29&page=", 0:82)

yem_links <- bind_rows(map(yem_urls, possibly(scrape_links, NA))) 

# Written so that this step is saved 
# yem_links %>% 
#   write_csv("./data/reliefweb_yem_links.csv")

yem_links <- read_csv("./data/reliefweb_yem_links.csv")

yem_links <- yem_links %>% 
  rename(url = value) %>% 
  pull(url) 

yem_a <- yem_links[1:400]
yem_a <- yem_a[-c(249, 250, 398, 399)]

yem_b <- yem_links[401:800]
yem_b <- 
yem_c <- yem_links[801:1200]
yem_d <- yem_links[1201:1644]

yem_a[108] %>% 
  map_df(., possibly(scrape_page, character(0), quiet = FALSE))

yem_a_scraped <- yem_a %>% 
  map_df(., possibly(scrape_page, character(0), quiet = FALSE))


yem_a2 <- yem_links[201:300]

yem_a_scraped %>% 
  group_by(link) %>% 
  slice(1) %>% 
  ungroup()

```


### Working script
These are the components of the core scraping function 
Assembled individually here to ensure that each functions
I must say, however, that I need to pay more attention to the selections from the SelectorGadget. 

```{r}

ukr_scraped <- tibble()

url <- "https://reliefweb.int/report/ukraine/war-ukraine-situation-report-who-ukraine-country-office-issue-no38-30-december-2022"

page <- read_html(url)

title <- page %>% html_nodes(".rw-page-title") %>% html_text2()
type <- page %>% html_nodes(".rw-article__header--with-meta .rw-entity-meta__tag-value--simple") %>% html_text2()
body <- page %>% html_nodes(".rw-report__content p") %>% html_text2() %>% as_tibble()
source <- page %>% html_nodes(".rw-article__header--with-meta .rw-entity-meta__tag-link") %>% html_text2()
date <- page %>% html_nodes(".rw-article__header--with-meta .rw-entity-meta__tag-value--published time") %>% html_text2()
theme <- page %>% html_nodes(".rw-entity-meta__tag-value--theme") %>% html_text2()

ukr_scraped <- tibble(title, type, source, theme, date, body)

```

## A fix for your stupidity

So, let us first scrape the addresses of the french and spanish articles so that they may be removed frm the dataset. 

We're going to make use of scrape_links() again 

```{r}
spanish_links <- paste0("https://reliefweb.int/updates?advanced-search=%28PC241%29_%28L269%29_%28DA20220101-20230104%29&page=", 0:1)

french_links <- paste0("https://reliefweb.int/updates?advanced-search=%28PC241%29_%28L268%29_%28DA20220101-20230104%29&page=", 0:1)

rbind(bind_rows(map(spanish_links, possibly(scrape_links, NA))),
      bind_rows(map(french_links, possibly(scrape_links, NA)))) %>% 
  write_csv("./data/links_to_remove.csv")

```


```{r}
# The language tag isn't really necessary now, just bear this in mind for the future
language <- page %>% html_nodes(".rw-entity-meta__tag-value--last .rw-entity-meta__tag-link") %>% 
  html_text2()
```

## Scrape FTS

```{r}
#block-fts-public-content a

scrape_links <-  function(page){ 
  
  Sys.sleep(1)
  
  page %>% 
    read_html() %>% 
    html_nodes(".rw-river-article__title a") %>% 
    html_attr("href") %>% 
    as_tibble()
}

```

```{r}
fts_country_links <- read_html("https://fts.unocha.org/countries/overview") %>% 
  html_nodes("#block-fts-public-content a") %>% 
  html_attr("href") %>% 
  as_tibble()
```

Think about this programmatically 


```{r}

# This unfortunately only pulls the first page of the table, 
# and you'd need to loop through, complicating factors is tha t
read_html("https://fts.unocha.org/countries/1/flows/2023?page=0#search-results") %>% 
  html_nodes("div.view-content.row") %>% 
  html_table() %>% .[[1]] %>% glimpse()
```

Figure out how to do to this properly 

```{r}
/v1/public/fts/flow?year=2016

res <-  GET("https://api.hpc.tools/public/fts/flow", 
            )

url <- "https://api.hpc.tools"
path <- "v1/public/fts/flow" 

res <- GET(url = url, path = path,
           query = list(year=2022, format = "xml"))

GET(url = url, path = "")

df <- fromJSON(rawToChar(res$content))

df %>% 
  unnest(destinationObjects)

```

