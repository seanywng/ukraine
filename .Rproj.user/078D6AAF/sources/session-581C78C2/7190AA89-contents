---
title: "Untitled"
output: html_document
date: '2023-01-06'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.width=9, message = FALSE, warning=FALSE)

library(tidyverse)
library(tidytext)
library(tidymodels)
library(broom)
library(ggraph)
library(tidylo)
library(widyr)
library(janitor)
library(lubridate)
library(SnowballC)
library(magrittr)
library(patchwork)
library(tidylo)
library(DT)
library(readxl)
library(flextable)
library(viridis)
library(scales)

`%out%` <- Negate(`%in%`)

range_wna <- function(x){(x-min(x, na.rm = TRUE))/(max(x, na.rm = TRUE)-min(x, na.rm = TRUE))}

theme_set(theme_light())

options(scipen = 100)

```


## Data



```{r}

ukr_links <- read_csv("./data/reliefweb_ukr_links.csv") %>% 
  rename(url = value) %>% 
  mutate(id = row_number())

# This is the list of Spanish and French links 
to_remove <- read_csv("./data/links_to_remove.csv") %>% 
  rename(url = value) %>% 
  pull(url)

rw <- read_csv("./data/scraped_full_20230101_20220101.csv") %>% 
  mutate(date = dmy(date), 
         month = month(date)) %>% 
  left_join(ukr_links, by = c("link" = "url")) %>%
  filter(link %out% to_remove) 

```

```{r}
rw_sector <- read_csv("./data/scraped_full_20230101_20220101.csv") %>% 
  mutate(date = dmy(date), 
         month = month(date)) %>% 
  left_join(ukr_links, by = c("link" = "url")) %>%
  filter(link %out% to_remove) %>% 
  separate_rows(theme, sep = "\n") %>%
  mutate(sector = tolower(theme)) %>% 
  select(-theme)
```



```{r}
top_40 <- rw %>% 
  count(source, sort = TRUE) %>% 
  filter(n >= 25) %>% 
  pull(source)

```


```{r}
rw %>% 
  count(type, sort = TRUE) %>% 
  ggplot(aes(x = n, 
             y = fct_reorder(type, n))) + 
  geom_col(aes(fill = type), 
           show.legend = FALSE) + 
  geom_text(aes(label = n),
            size = 3, 
            hjust = "inward")
```


## Titles 

```{r}
titles <- rw %>% 
  filter(source %out% c("ACLED", "OSCE", "IAEA")) %>% 
  select(id, title) %>% 
  unnest_tokens(word, title) %>% 
  count(id, word, sort = TRUE) %>% 
  anti_join(stop_words, by = "word") %>% 
  mutate(word = str_remove_all(word, "'")) %>% 
  mutate(stem = wordStem(word, language = "porter")) %>% 
  filter(stem %out% c("de", "des", "la", "las", "le", "les", 
                      "en", "el", "http", "se", "ses")) %>% 
  mutate(stem = recode(stem, 
                       "ukraine’" = "ukrain")) %>% 
  filter(nchar(stem) < 20)
  
```


```{r}
set.seed(2023)

titles %>% 
  pairwise_count(word, id, sort = TRUE, upper = FALSE) %>%
  filter(str_detect(item1, "[a-z]") & str_detect(item2, "[a-z]")) %>%
  filter(n >= 30) %>%
  igraph::graph_from_data_frame() %>% 
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "cyan4") +
  geom_node_point(size = 2) +
  geom_node_text(aes(label = name), repel = TRUE, size = 4, 
                 point.padding = unit(0.2, "lines")) + 
  labs(title = "Word pairs in Reliefweb Ukraine articles",
       subtitle = "2022-01-01 to 2023-01-01")

ggsave("./plots/title_network_graph.png", width = 42, height = 29.7, 
       units = "cm", dpi = 400)
```

Plot is saved, so you seem to be happy with it 

## Words 

You wrote this all better in th e
Reliefweb, in general, is full of fairly repetitive content.
Text mining algorithms have uncovered a substantial amount of boilerplate. 
This is why we examine the term frequency (the number of times a word appears in the corpus) and the term frequency-inverse document frequency (tf-idf), which balances the most common words against the words tha

I think you should remove OSCE, ACLED and IAEA from the corpus, since these have been included in the ACLED database anyway 
Furthermore, if one examines the ACLED entries, one notes that 

```{r}

words <- rw %>% 
  select(id, source, body) %>% 
  # filter(source %out% c("ACLED", "OSCE", "IAEA")) %>% 
  unnest_tokens(word, body) %>% 
  # mutate(word = str_remove_all(word, " \\'s")) %>%
  count(id, word, sort = TRUE) %>% 
  anti_join(stop_words, by = "word") %>% 
  filter(str_detect(word, "[a-z]")) %>%
  filter(!str_detect(word, "\\.(com|org|edu|gov)")) %>% 
  mutate(stem = wordStem(word, language = "porter")) %>% 
  filter(stem %out% c("de", "des", "la", "las", "le", "les", 
                      "en", "el", "http", "se", "ses")) %>% 
  # filter(!str_detect(word, "ukraine|ukrainian|people|humanitarian|support|including|country|\\.")) %>% 
  # mutate(stem = recode(stem, 
  #                      "ukraine’" = "ukrain")) %>% 
  filter(nchar(stem) < 20) 

words %>% 
  left_join(rw %>% 
              select(id, source), by = "id") %>% 
  count(source, word) %>% 
  add_count(word, wt = n, name = "word_total")
  


```


### Word pair network graph

However, what we can see is that all these most common words are largely boilerplate. Though "systems" and "supplies" might lead 
children, humanitarian assistance, displaced
Really, what else was gonna be said 

```{r}
set.seed(2023)

words_to_remove <- c("ukraine", "ukranian", "ukrainian", "humanitarian", "support", "people", 
                     "including", "international", "countries", "country")

# Be aware that word_pairs as it is will return a df of 111 million rows 
# Consider setting a better threshold
word_pairs <- words %>%
  # filter(word %out% words_to_remove) %>% 
  pairwise_count(word, source, sort = TRUE, upper = FALSE) %>% 
  filter(str_detect(item1, "[a-z]") & str_detect(item2, "[a-z]")) %>% 
  filter(n > 100)


word_pair_network_graph <- words %>%
  # filter(word %out% words_to_remove) %>% 
  pairwise_count(word, id, sort = TRUE, upper = FALSE) %>%
  filter(str_detect(item1, "[a-z]") & str_detect(item2, "[a-z]")) %>%
  # Start at 500, remember how many pairs there are 
  filter(n > 500) %>% 
  igraph::graph_from_data_frame() %>% 
  ggraph(layout = "stress") +
  geom_edge_link(aes(edge_alpha = n/2, edge_width = n), edge_colour = "tomato") +
  geom_node_point(size = 2) +
  geom_node_text(aes(label = name), repel = TRUE, size = 3, 
                 point.padding = unit(0.2, "lines")) +
  theme_void() + 
  labs(title = "Most common word pairs in Reliefweb articles on Ukraine", 
       subtitle = "2022-01-01 to 2023-01-01; lines show number of co-occurences", 
       edge_width = "") + 
  # Removing the legends for edge width and edge alpha 
  guides(edge_alpha = "none", 
         edge_width = "none") + 
  theme(plot.background = element_rect(fill = "white"))
  

ggsave(word_pair_network_graph, 
       filename = "./plots/word_pair_network_graph2.png",
       dpi = 400, width = 16.5, height = 11.7, units = "in") 

word_pairs %>% 
  filter(n >= 800) %>% 
  arrange(desc(n))


```


### Tf-idf words

Unfortunately, the tf-idf for words is not exactly that useful 
The interpretabilty at the bigram level is just much better 

Stop polluting your environment

```{r}
tf_idf_words <- words_source %>% 
  bind_tf_idf(source, word, n) %>% 
  bind_log_odds(source, word, n) %>% 
  arrange(desc(tf_idf)) 
  filter(word_total > 2)


```


```{r}
tf_idf_words <- words %>% 
  bind_tf_idf(word, id, n) %>% 
  bind_log_odds(word, id, n) %>% 
  arrange(desc(tf_idf)) %>% 
  left_join(rw %>% select(id, source), 
            by = "id") %>%  
  # n > 1 gives the most interpretable results, 
  # though bigrams are still more useful by far
  filter(n > 1)


tf_idf_alt %>% 
  ggplot(aes(x = word_total)) + 
  geom_histogram() + 
  scale_x_log10() + 
  scale_y_log10() + 
  geom_vline(xintercept = 2, lty = 2, colour = "red") + 
  geom_vline(xintercept = 3, lty = 2, colour = "orange") + 
  labs(title = "Possible cut-offs for tf-idf words")
```



### Log odds

Not really needed, maybe later? 

```{r}
log_odds <- bigrams %>% 
  bind_log_odds(bigram, source, n) %>% 
  filter(!is.infinite(log_odds_weighted))

log_odds_source <- function(tbl, source) {
  
  source <- enquo(source)
  sourcen <- quo_name(source)
  
  tbl %>%
    filter(source == !!source) %>%
    arrange(desc(log_odds_weighted)) %>%
    head(15) %>%
    ggplot(aes(x = log_odds_weighted,
               y = fct_reorder(word, log_odds_weighted))) +
    geom_col(show.legend = FALSE,
             fill = "cornflowerblue") +
    labs(x = "Log odds",y = "",
      title = paste0("Term frequency -- ", sourcen),
      subtitle = "2022-01-01 to 2023-01-01")
}


log_odds %>% 
  filter(source %in% c("USAID", "EC", "ECHO")) %>% 
  group_by(source) %>% 
  slice_max(log_odds_weighted, n = 15) %>% 
  ungroup() %>% 
  ggplot(aes(x = log_odds_weighted,
               y = fct_reorder(word, log_odds_weighted))) +
  geom_col(show.legend = FALSE,
           aes(fill = source)) +
  facet_wrap(~ source, scales = "free") + 
  labs(x = "Log odds",y = "",
       title = "Log odds -- selected actors",
       subtitle = "2022-01-01 to 2023-01-01")

log_odds %>% 
  filter(source == "WFP") %>% 
  arrange(desc(log_odds_weighted))
  

```


### Save word plots 

We will run this at least twice, first at n > 0, then n > 1, then n > 2 
try .fun = mean to see what happens 


```{r}
save_words <- function(df, filename) {
  
  word_patchwork <- df %>% 
    group_by(word) %>% 
    slice_max(tf, n = 1) %>% 
    ungroup() %>% 
    arrange(desc(tf)) %>% 
    head(20) %>% 
    ggplot(aes(x = tf, 
               y = fct_reorder(word, tf, .fun = mean)))+
    geom_col(show.legend = FALSE, 
             fill = "cornflowerblue") + 
    labs(x = "Term frequency", y = "", 
         title = paste0("Term frequency: ", filename), 
         subtitle = "2022-01-01 to 2023-01-01") + 

  df %>%
    group_by(word) %>% 
    slice_max(tf_idf, n = 1) %>% 
    ungroup() %>% 
    arrange(desc(tf_idf)) %>% 
    head(20) %>%
    ggplot(aes(x = tf_idf, 
               y = fct_reorder(word, tf_idf, .fun = mean)))+
    geom_col(show.legend = FALSE, 
             fill = "tomato") + 
    labs(x = "Tf-idf", y = "", 
         title = paste0("Tf-idf: ", filename), 
         subtitle = "2022-01-01 to 2023-01-01") +
  
  df %>%
    filter(!is.infinite(log_odds_weighted)) %>% 
    group_by(word) %>% 
    slice_max(log_odds_weighted, n = 1) %>% 
    ungroup() %>% 
    arrange(desc(log_odds_weighted)) %>% 
    head(20) %>%
    ggplot(aes(x = log_odds_weighted, 
               y = fct_reorder(word, log_odds_weighted, .fun = mean)))+
    geom_col(show.legend = FALSE, 
             fill = "forestgreen") + 
    labs(x = "Log-odds", y = "", 
         title = paste0("Log-odds: ", filename), 
         subtitle = "2022-01-01 to 2023-01-01")
  
  ## SET the correct directory
  ggsave(plot = word_patchwork,
         filename = paste0("./plots/tf_idf_word_filter1/",filename, ".png"),
         device = "png",
         dpi = 300, height = 11, width = 14, units = "in")
  
}
```


```{r}
more_than_5 <- rw %>% 
  count(source, sort = TRUE) %>% 
  filter(n >= 5) %>% 
  pull(source)
```


We will run this at least twice, first at n > 0, then n > 1, then n > 2 
try .fun = mean to see what happens 

```{r}
# Throws an error, but still works 
# This error occurs when you ask R to overwrite an existing file 
tf_idf_words %>% 
  filter(source %in% more_than_5) %>% 
  nest(-source) %$%
  walk2(data, source, save_words)
```

```{r}
rw %>% 
  count(source, sort = TRUE)
```


I guess IMC really are CIA 

```{r}

# Article checking function

rw %>% 
  # Including tolower has significantly impacted performance
  filter(source == "UNFPA" & str_detect(body, "Action Plan")) %>%
  sample_n(1) %>% 
  pull(body, title)

rw %>% 
  filter(id == 
           tf_idf %>%
           filter(source == "ACAPS" & word == "slipping") %>%
           sample_n(1) %>%
           pull(id)) %>%
  pull(body)

ukr_links %>% filter(id == 2094) %>% pull(url)
```




## Bigrams


```{r}
bigrams <- rw %>% 
  # You're filtering out one clump here 
  # filter(source %out% c("IAEA", "ACLED")) %>% 
  select(source, id, body) %>% 
  unnest_tokens(bigram, body, token = "ngrams", n = 2) %>% 
  filter(!is.na(bigram)) %>% 
  separate(bigram, c("word1", "word2"), sep = " ") %>% 
  mutate(word = str_remove_all(word1, " \\'s"), 
         word2 = str_remove_all(word2, "\\'s")) %>%
  filter(!word1 %in% stop_words$word & (nchar(word1) < 20)) %>% 
  filter(!word2 %in% stop_words$word & (nchar(word2) < 20)) %>% 
  filter(str_detect(word1, "[a-z]") & !str_detect(word1, "<")) %>% 
  filter(str_detect(word2, "[a-z]") & !str_detect(word2, "<")) %>% 
  filter(!str_detect(word1, "\\.(com|org|edu|gov)") & 
           !str_detect(word2, "\\.(com|org|edu|gov)")) %>% 
  unite(bigram, word1, word2, sep = " ")
  
# bigram_counts <- rw %>% 
#   # You're filtering out one clump here 
#   # filter(source %out% c("IAEA", "ACLED")) %>% 
#   select(source, id, body) %>% 
#   unnest_tokens(bigram, body, token = "ngrams", n = 2) %>% 
#   filter(!is.na(bigram)) %>% 
#   separate(bigram, c("word1", "word2"), sep = " ") %>% 
#   filter(!word1 %in% stop_words$word & (nchar(word1) < 20)) %>% 
#   filter(!word2 %in% stop_words$word & (nchar(word2) < 20)) %>% 
#   filter(str_detect(word1, "[a-z]") & !str_detect(word1, "<")) %>% 
#   filter(str_detect(word2, "[a-z]") & !str_detect(word2, "<")) %>% 
#   count(word1, # word2, sort = TRUE)

# You might actually have gotten the argument order wrong 
# You were not supplying the feature 

  

```

### Tf-idf bigrams 

```{r}
tf_idf_bigrams <- bigrams %>%
  count(id, bigram) %>% 
  bind_tf_idf(bigram, id, n) %>% 
  bind_log_odds(bigram, id, n) %>% 
  arrange(desc(tf_idf)) %>% 
  left_join(rw %>% 
              select(id, source), 
            by = "id") %>% 
  filter(bigram %out% c("million people", "humanitarian assistance")) %>% 
  rename(word = bigram)  
  filter(n > 1)

# Cut-off point checker
tf_idf_bigrams_odds %>% 
  ggplot(aes(x = n)) + 
  geom_histogram() + 
  scale_x_log10() + 
  scale_y_log10() + 
  geom_vline(xintercept = 2, lty = 2, colour = "red") + 
  geom_vline(xintercept = 3, lty = 2, colour = "orange") + 
  labs(title = "Possible cut-off points for bigrams")


```

I find it very puzzling that this dataset should end up like this. I fail to understand why the bigram  

```{r}
tf_idf_bigrams_odds <- bigrams %>% 
  count(id, bigram) %>% 
  bind_tf_idf(bigram, id, n) %>% 
  bind_log_odds(bigram, id, n) %>% 
  arrange(desc(tf_idf)) %>% 
  left_join(rw %>% 
              select(id, source), 
            by = "id") %>% 
  rename(word = bigram) %>% 
  add_count(word, wt = n, name = "word_total") %>% 
  # group_by(source) %>% 
  # mutate(max_log_odds_weighted = 
  #          max(log_odds_weighted[!is.infinite(log_odds_weighted)], na.rm = TRUE)) %>% 
  # mutate(log_odds_weighted = ifelse(is.infinite(log_odds_weighted), 
  #                                   max(max_log_odds_weighted), 
  #                                   log_odds_weighted)) %>% 
  # ungroup() %>%  
  filter(n > 1)

tf_idf_bigrams_odds %>% 
  write_csv("./data/tf_idf_bigrams_odds.csv")
```


### Months 

```{r}
tf_idf_bigram_months <- rw %>% 
  filter(source %out% c("IAEA", "ACLED", "OSCE")) %>% 
  select(id, month, body) %>% 
  unnest_tokens(bigram, body, token = "ngrams", n = 2) %>% 
  filter(!is.na(bigram)) %>% 
  separate(bigram, c("word1", "word2"), sep = " ") %>% 
  filter(!word1 %in% stop_words$word & (nchar(word1) < 20)) %>% 
  filter(!word2 %in% stop_words$word & (nchar(word2) < 20)) %>% 
  filter(str_detect(word1, "[a-z]") & !str_detect(word1, "<")) %>% 
  filter(str_detect(word2, "[a-z]") & !str_detect(word2, "<")) %>% 
  unite(bigram, word1, word2, sep = " ") %>% 
  count(month, bigram) %>% 
  bind_tf_idf(bigram, month, n) %>% 
  bind_log_odds(bigram, month, n) %>% 
  arrange(desc(tf_idf)) %>% 
  filter(bigram %out% c("million people", "humanitarian assistance")) %>% 
  rename(word = bigram) %>% 
  filter(n > 1) %>% 
  ungroup()


```


This is actually fairly interpretable. 

```{r}

bigram_months <- tf_idf_bigram_months %>%
  filter(!is.infinite(log_odds_weighted)) %>%
  mutate(month = factor(month.abb[month],levels = month.abb)) %>%
  group_by(month) %>% 
  arrange(desc(log_odds_weighted), desc(tf_idf)) %>% 
  mutate(rank = row_number()) %>% 
  filter(rank <= 20) %>% 
  ungroup() %>% 
  ggplot(aes(x = log_odds_weighted, 
             y = fct_reorder(word, log_odds_weighted), 
             fill = month)) + 
  geom_col(show.legend = FALSE) +
  scale_fill_viridis_d() + 
  facet_wrap(~ month, scales = "free") + 
  labs(x = "Log-odds", 
       y = "", 
       title = "Reliefweb Ukraine articles: log-odds of word specificity by month", 
       subtitle = "A year of humanitarian rhetoric") +
   theme(legend.position = "none",
        axis.text.x = element_text(size = 5, hjust = 1, vjust = 1), 
        axis.text.y = element_text(size = 8), 
        strip.text = element_text(size = 9, face = "bold"),
        strip.background = element_rect(fill = "#212121")) 

 ggsave(plot = bigram_months,
         filename = paste0("./plots/bigram_months_bar.png"),
         device = "png",
         dpi = 300, height = 11, width = 14, units = "in")


```

```{r}
rw %>% 
  filter(month == 6 & str_detect(body, "mental toll") ) %>% 
  sample_n(1) %>% 
  pull(body, link)

# Stronger checking function, which will always work, unlike the one above
rw %>% 
  filter(id == 
           bigrams_month %>%
           filter(month == 8 & bigram  == "operationally feasible") %>%
           sample_n(1) %>%
           pull(id)) %>%
  # Is this a fluke? How am I able to pull two columns?
  pull(body, source) 

```




### Save bigram plots 

Interpretability is very high on the bigram dataset, however, only the broadest strokes are noticed. One clear example is OCHA's result. 

Term frequency here shows us the phrases that each organisation uses most commonly. Often, their name or their mandate ("food assistance") will appear in this list. 

Tf-idf (term frequency-inverse document frequency) is another measure by which significant words are identified. The term frequency -- the number of times a term appears in the corpus -- is tempered by the inverse document frequency, which discounts words that tend to appear again and again (think "humanitarian assistance"). The combined metric is useful for determining which words are common, but not too common. A suspicious-minded person might even say that this is where we may find what agencies really care about, as opposed to the boilerplate that they include in every report. 

Finally, we also evaluate the corpuses by looking at their log odds. This highlights words that are more likely to originate from a source over any other sources. This is where we may see the unique information that each source brings to the table.  


try .fun = mean to see what happens 


```{r}
save_bigrams <- function(df, filename) {
  
  bigram_patchwork <- df %>% 
    group_by(word) %>% 
    slice_max(tf, n = 1) %>% 
    ungroup() %>% 
    arrange(desc(tf)) %>% 
    head(20) %>% 
    ggplot(aes(x = tf, 
               y = fct_reorder(word, tf, .fun = mean)))+
    geom_col(show.legend = FALSE, 
             fill = "cornflowerblue") + 
    labs(x = "Term frequency", y = "", 
         title = paste0("Term frequency: ", filename), 
         subtitle = "2022-01-01 to 2023-01-01") + 

  df %>%
    group_by(word) %>% 
    slice_max(tf_idf, n = 1) %>% 
    ungroup() %>% 
    arrange(desc(tf_idf)) %>% 
    head(20) %>%
    ggplot(aes(x = tf_idf, 
               y = fct_reorder(word, tf_idf, .fun = mean)))+
    geom_col(show.legend = FALSE, 
             fill = "tomato") + 
    labs(x = "Tf-idf", y = "", 
         title = paste0("Tf-idf: ", filename), 
         subtitle = "2022-01-01 to 2023-01-01") +
  
  df %>% 
    filter(!is.infinite(log_odds_weighted)) %>% 
    group_by(word) %>% 
    slice_max(log_odds_weighted, n = 1) %>% 
    ungroup() %>% 
    arrange(desc(log_odds_weighted)) %>% 
    head(20) %>%
    ggplot(aes(x = log_odds_weighted, 
               y = fct_reorder(word, log_odds_weighted, .fun = mean)))+
    geom_col(show.legend = FALSE, 
             fill = "forestgreen") + 
    labs(x = "Log odds", y = "", 
         title = paste0("Log odds: ", filename), 
         subtitle = "2022-01-01 to 2023-01-01")
  
  ggsave(plot = bigram_patchwork,
         filename = paste0("./plots/bigram_source_odds1/",filename, ".png"),
         device = "png",
         dpi = 300, height = 11, width = 14, units = "in")
  
}
```

```{r}
more_than_5 <- rw %>% 
  count(source, sort = TRUE) %>% 
  filter(n >= 5) %>% 
  pull(source)
```


```{r}
# Throws an error, but still works 
# This error occurs when you ask R to overwrite an existing file 
tf_idf_bigrams_odds %>% 
  filter(source %in% more_than_5) %>% 
  nest(-source) %$%
  walk2(data, source, save_bigrams)
```

#### Search bigrams 

Congratulations for cheating hahahahhaa and figuring out an easy way to do this 


```{r}
tf_idf_bigrams_odds %>%
    filter(source %in% more_than_5) %>%
  add_count(word, wt = n, name = "word_total") %>% 
  filter(word_total > 5) %>% 
    filter(!is.infinite(log_odds_weighted)) %>%
    distinct(source, word, .keep_all = TRUE) %>%  
  ungroup()
  top_n(1)
    slice_max(log_odds_weighted, n = 10) %>%
    distinct(id, word, .keep_all = TRUE) %>% 
    ungroup()
```


```{r}

rbind(
  # Pay attention to the first one since the log-odds filtering is
  # so finnicky
  tf_idf_bigrams_odds %>%
    filter(source %in% more_than_5) %>%
    add_count(word, wt = n, name = "word_total") %>%
    filter(word_total > 10) %>%
    filter(!is.infinite(log_odds_weighted)) %>%
    distinct(source, word, .keep_all = TRUE) %>%
    group_by(source) %>% 
    slice_max(log_odds_weighted, n = 10) %>% 
    ungroup(),
  
  tf_idf_bigrams_odds %>%
    filter(source %in% more_than_5) %>%
    group_by(source) %>%
    slice_max(tf, n = 20) %>%
    ungroup(),
  
  tf_idf_bigrams_odds %>%
    filter(source %in% more_than_5) %>%
    group_by(source) %>%
    slice_max(tf_idf, n = 20) %>%
    ungroup()
  
) %>%
  left_join(rw %>% 
              select(id, link, body), 
            by = "id") %>%  
  mutate(body = str_sub(body, 1, 150)) %>% 
  distinct(word, id, source, tf_idf, .keep_all = TRUE) %>%  
  arrange(desc(n), desc(tf_idf)) %>% 
  select(source, bigram = word, count = n, `text (1st 150 char)` = body, link, -tf_idf) %>%  
  datatable(filter = list(position = "top", clear = TRUE), 
            options = list(pageLength = 10, 
                           scrollX = TRUE,
                           autoWidth = TRUE, 
                           columnDefs = list(
                             list(width = "80px", targets = 1:3),
                             list(width = "180px", targets = 4), 
                             list(width = "100px", targets = 5)),
                           search = list(regex = TRUE)
                           
                           ,
                                           initComplete = htmlwidgets::JS(
          "function(settings, json) {",
          paste0("$(this.api().table().container()).css({'font-size': '", "8.5pt", "'});"),
          "}")
       ),
  caption = htmltools::tags$caption(style = 'caption-side: top; 
                                    text-align: center; 
                                    color:black; font-size:140% ;',
                                    "Bigram search helper")
     )  
  # formatRound(c("correlation"), digits = 3)

```

> topic modelling for NGOs
who's related with programme quality
who's just in and out maybe it'll be useful

### Source notes
So, I think, for the best results, you should separate the sources into two categories, one with many 

Interpretability is very high on the bigram dataset, however, only the broadest strokes are noticed. One clear example is OCHA's result. 

```{r}

# Article checking function
rw %>% 
filter(source == "WFP" & str_detect(body, "agency convoys")) %>%
  sample_n(1) %>% 
  pull(body, id)

rw %>% 
  filter(id == 
           tf_idf %>%
           filter(word == "gou") %>%
           sample_n(1) %>%
           pull(id)) %>%
  # Is this a fluke? How am I able to pull two columns?
  pull(body, source)

ukr_links %>% filter(id == 2118) %>% pull(url)


# alternative that uses id
# this one always works 
# this is slower (by 10ms) and uses a bit more memory, 
# assuming you understand anything at all about profvis 
rw %>% 
  filter(id == 
           tf_idf_bigrams %>%
           filter(source == "ECHO" & word == "ucpm hubs") %>%
           sample_n(1) %>%
           pull(id)) %>%
  pull(body, date)

```

What is the game the EC is playing with the French presidency? Macron is a backstabbing fool who already tried to go around the Americans to get a deal with the Russians. My assumption here is that the EC is humouring French attempts to return to Great Power status. 

```{r}
tf_idf_bigrams %>% 
  filter(bigram %out% c("million people", "humanitarian assistance")) %>% 
  rename(word = bigram) %>% 
bigram_frequency_by_source("EC")
```

Ukraine's only avenue for agricultural and non-agricultural exports is the EU? 

We can also see the World Bank being mostly inactive. There are no mentions of credit or financing -- without that, their influence is diminished. They do mention the phrase "future assessments". Strangely also, they have publicised the 

We can also almost definitely ignore the World Bank for the moment as well. I see no mentions of credit or financing. With the phrase "future assessments" coming at the list of log odds. I'd be interested in seeing what their relationship is with the EIB. 

I wouldn't use reliefweb articles to parse the thinking of the Ukrainian government. 

```{r}
tf_idf_bigrams %>% 
  filter(bigram %out% c("million people", "humanitarian assistance")) %>% 
  rename(word = bigram) %>% 
bigram_frequency_by_source("World Vision")
```

We include World Vision because they have the clearest indication, from any NGO, of a multi-sector programme. Developing a list of

MercyCorps uses certain programme quality keywords in the documents submitted to Reliefweb
Words such as prioritizing, underlying conditions, inform programme 
sectoral
preliminary 

Is there a relationship with agencies that use these programme quality keywords? Truly up to you to find out. What I can say is that it will be a damn sight better than those that don't mention it at all. 

Next we just need to find agencies that are adaptive and accountable,

> A cheat search engine function -- it's an html that just has one DT per source, and all 
> Or... a 

```{r}

create_bigrams <- function(df, filename) {
  
  filename <- enquo(filename)
  filenamen <- quo_name(filename)
  
  df %>% 
    filter(source == !!filename) %>% 
    group_by(word) %>% 
    slice_max(tf, n = 1) %>% 
    ungroup() %>% 
    arrange(desc(tf)) %>% 
    head(20) %>% 
    ggplot(aes(x = tf, 
               y = fct_reorder(word, tf, .fun = mean)))+
    geom_col(show.legend = FALSE, 
             fill = "cornflowerblue") + 
    labs(x = "Term frequency", y = "", 
         title = paste0("Term frequency: ", filenamen), 
         subtitle = "2022-01-01 to 2023-01-01") + 

  df %>%
    filter(source == !!filename) %>% 
    group_by(word) %>% 
    slice_max(tf_idf, n = 1) %>% 
    ungroup() %>% 
    arrange(desc(tf_idf)) %>% 
    head(20) %>%
    ggplot(aes(x = tf_idf, 
               y = fct_reorder(word, tf_idf, .fun = mean)))+
    geom_col(show.legend = FALSE, 
             fill = "tomato") + 
    labs(x = "Tf-idf", y = "", 
         title = paste0("Tf-idf: ", filenamen), 
         subtitle = "2022-01-01 to 2023-01-01") +
  
  df %>% 
    filter(source == !!filename) %>% 
    filter(!is.infinite(log_odds_weighted)) %>% 
    group_by(word) %>% 
    slice_max(log_odds_weighted, n = 1) %>% 
    ungroup() %>% 
    arrange(desc(log_odds_weighted)) %>% 
    head(20) %>%
    ggplot(aes(x = log_odds_weighted, 
               y = fct_reorder(word, log_odds_weighted, .fun = mean)))+
    geom_col(show.legend = FALSE, 
             fill = "forestgreen") + 
    labs(x = "Log odds", y = "", 
         title = paste0("Log odds: ", filenamen), 
         subtitle = "2022-01-01 to 2023-01-01")
  }
```


```{r}
tf_idf_bigrams_odds %>% 
  create_bigrams("CCCM Cluster") 

tf_idf_bigrams_odds %>% 
  filter(source == "CCCM Cluster") %>% 
  arrange(desc(log_odds_weighted), desc(word_total))
```

Maybe Health Cluster is a better example to start with,

The high-ranking tf and tf-idf bigrams in CCCM point very clearly towards "collective sites" and "communal settings", furthermore, when examining the list of log-odds terms, we see many ties for first place, indicating that the CCCM corpus is full of terms very rarely used by other actors. It is hoped that by drilling down, we will see the contents and details of what CCCM is saying when they mention "css reported", "site monitoring" and "reportedly received". It appears that there is a richness here rare in other the other actors that submitted articles to reliefweb. 

![](./plots/bigram_source_odds/CCCM Cluster.png)


In contrast, we see Christian Aid, which is, in my opinion, one of the last sources I would look to for relevant data. We can see that their tf and tf-idf are dominated by "Crown Agents". Looking at their log-odds, we see mentions of "community shelter", but the real meat of their corpus can be inferred from "uk public", "committee dec", "british public" and "donating money". Perhaps if I were more interested in aid priorities. 

![](./plots/bigram_source_odds/Christian Aid.png)

Another good example of an actor with a varied vocabulary, without much boilerplate, is the Health Cluster. Analytics, a focus on data and even mentions of coverage -- this is a cluster that cares about quality, at least on paper. 

![](./plots/bigram_source_odds/Health Cluster.png)

The Ukrainian government documents within reliefweb are likely not representative of the Ukrainian government's priorities. Or are we to understand that the Ukrainian government prefers humanitarian actors to intervene in the education cluster.

![](./plots/bigram_source_odds/Govt. Ukraine.png)

 




## Network graphs

### Bigrams

```{r}
set.seed(2023)

network_graph <- bigrams %>% 
  filter(bigram %out% c("million people", 
                        #"internally displaced", 
                        "united nations", 
                        #"human rights", 
                        #"humanitarian assistance",
                        "february 2022")) %>% 
  distinct(id, bigram) %>% 
  add_count(bigram) %>% 
  filter(n >= 50) %>% 
  pairwise_cor(bigram, id, sort = TRUE) %>% 
  filter(correlation >= .15) %>% 
  igraph::graph_from_data_frame() %>% 
  ggraph(layout = "kk") + 
  geom_edge_fan(aes(alpha = correlation),
                colour = "cornflowerblue",
                check_overlap = TRUE,
                width = .1) + 
  scale_alpha_continuous(range = c(.1, .4)) + 
  geom_node_point(colour = "black", alpha = .1, size = .5) + 
  geom_node_text(aes(label = name), size = 1, 
                 vjust = 1, hjust = 1, 
                 check_overlap = TRUE) +
  theme(legend.position = "none") + 
  labs(title = "Network graphs of Reliefweb bigrams related to the Ukraine conflict in 2022", 
       subtitle = "Data source: https://reliefweb.int/updates?advanced-search=%28PC241%29_%28DA20220101-20230104%29")

ggsave("./plots/network_graph_kk_full.png", network_graph, width = 42, height = 29.7, units = "cm", dpi = 400)
  
```

### Words

There really is quite limited utility in a network graph based on tokens unnested by source. 

```{r}
set.seed(2023)

network_graph <- words_id %>% 
  filter(word %out% words_to_remove) %>% 
  distinct(id, word) %>% 
  add_count(word) %>% 
  filter(n >= 200) %>% 
  pairwise_cor(word, id, sort = TRUE) %>% 
  filter(correlation >= .15) %>% 
  igraph::graph_from_data_frame() %>% 
  ggraph(layout = "fr") + 
  geom_edge_link(aes(edge_alpha = correlation, 
                     edge_width = correlation), 
                 edge_colour = "tomato") + 
  geom_node_point(size = 2) + 
  geom_node_text(aes(label = name), 
                 repel = TRUE, 
                 size = 2, 
                 point.padding = unit(0.2, "lines")) + 
  theme(legend.position = "none") + 
  labs(title = "Network graphs of Reliefweb bigrams related to the Ukraine conflict in 2022", 
       subtitle = "Data source: https://reliefweb.int/updates?advanced-search=%28PC241%29_%28DA20220101-20230104%29")

geom_edge_link(aes(edge_alpha = n/2, edge_width = n), edge_colour = "tomato") +
  geom_node_point(size = 2) +
  geom_node_text(aes(label = name), repel = TRUE, size = 3, 
                 point.padding = unit(0.2, "lines")) +
  theme_void() + 
  labs(title = "Most common word pairs in Reliefweb articles on Ukraine", 
       subtitle = "2022-01-01 to 2023-01-01; lines show number of co-occurences", 
       edge_width = "") + 
  # Removing the legends for edge width and edge alpha 
  guides(edge_alpha = "none", 
         edge_width = "none") + 
  theme(plot.background = element_rect(fill = "white"))


ggsave("./plots/network_graph_fr.png", network_graph, width = 42, height = 29.7, units = "cm", dpi = 400)

  
```

## Pairwise correlations 

```{r dt-pairwise-words}
words_id %>%  
  filter(word %out% words_to_remove) %>% 
  distinct(id, word) %>% 
  add_count(word) %>% 
  filter(n >= 120) %>% 
  pairwise_cor(word, id, sort = TRUE) %>%
  rename(word = item1, match = item2) %>% 
  group_by(word) %>% 
  top_n(correlation, n = 20) %>% 
  datatable(filter = list(position = "top", clear = FALSE), 
            options = list(pageLength = 10, scrollX = TRUE,
                           search = list(regex = TRUE)
  #                         
  #                         ,
  #                                         initComplete = htmlwidgets::JS(
  #        "function(settings, json) {",
  #        paste0("$(this.api().table().container()).css({'font-size': '", "8.5pt", "'});"),
  #        "}")
       ),
  caption = htmltools::tags$caption(style = 'caption-side: top; 
                                    text-align: center; 
                                    color:black; font-size:140% ;',
                                    "Pairwise correlations in Reliefweb Ukraine articles")
     ) %>% 
  formatRound(c("correlation"), digits = 3)

```

```{r dt-check-bigrams}

ukr_links %>% 
  filter(id == rw %>% 
           filter(str_detect(body, "Izium") & 
                    str_detect(body, "boys")) %>% 
           sample_n(1) %>% 
           pull(id)) %>%
  pull(url)
```


## LM 

Perhaps one of the most interesting things you can do is see if there is any relationship between the rhetoric each agency uses in reliefweb and the funding that it receives? 

## Sectors

```{r}

words_source <- rw %>% 
  select(id, source, body) %>% 
  filter(source %out% c("ACLED", "OSCE", "IAEA")) %>% 
  unnest_tokens(word, body) %>% 
  count(source, word, sort = TRUE) %>% 
  anti_join(stop_words, by = "word") %>% 
  filter(str_detect(word, "[a-z]")) %>%
  mutate(word = str_remove_all(word, "'")) %>% 
  mutate(stem = wordStem(word, language = "porter")) %>% 
  filter(stem %out% c("de", "des", "la", "las", "le", "les", 
                      "en", "el", "http", "se", "ses")) %>% 
  # filter(!str_detect(word, "ukraine|ukrainian|people|humanitarian|support|including|country|\\.")) %>% 
  # mutate(stem = recode(stem, 
  #                      "ukraine’" = "ukrain")) %>% 
  filter(nchar(stem) < 20)

words_sectors <- rw_sectors %>% 
  select(id, sector, body) %>% 
  

```

## FTS

Ok, a few things -- you're gonna need a lot more work in this section. You just only recently started looking at it anyway. Also, decide on whether to use the full country dataset or only the field cluster level 

```{r}
fts <- read_excel("./data/FTS_Ukraine_2022_flows_2022_as_on_2023-01-19.xlsx", 
           sheet = "Export data", 
           skip = 2) %>% 
  clean_names() 
  
```

```{r}
fts %>% glimpse()

# Not really that meaningful unless you set up a threshold for funds 
fts %>% 
  filter(amount_us >= 50000) %>% 
  pivot_longer(cols = c(source_org, destination_org), 
               names_to = "org_type", 
               values_to = "org") %>% 
  mutate(org_type = str_remove_all(org_type, "\\_org")) %>% 
  pairwise_cor(org, flow_id, sort = TRUE, upper = FALSE)

fts %>% 
  filter(source_org == "Monaco, Government of") 
  
fts %>% 
  pivot_longer(cols = c(source_org, destination_org), 
               names_to = "org_type", 
               values_to = "org") %>% 
  mutate(org_type = str_remove_all(org_type, "\\_org")) %>% 
  pairwise_count(org, flow_id, sort = TRUE, upper = FALSE, wt = amount_us) %>% 
  rename(org1 = item1, 
         org2 = item2, 
         USD = n) %>% 
  head(20) %>% 
  flextable() %>% 
  set_caption(caption = "Top donor and humanitarian agency pairs") %>% 
  theme_zebra() %>% 
  add_footer_lines("as of 18 January 2023")
    
    
```

```{r}
donor_cluster <- fts %>% 
  filter(!is.na(sector)) %>% 
  filter(!str_detect(sector, "\\|")) %>% 
  rbind(
    fts %>%
      filter(str_detect(sector, "\\|")) %>%  
      separate_rows(sector, sep = "\\|") %>%
      group_by(flow_id) %>% 
      mutate(num_sectors = n(), 
             amount_usd = amount_us, 
             amount_us = amount_usd / num_sectors) %>% 
      select(-num_sectors, -amount_usd) %>%
      ungroup()
    ) %>%
  mutate_at(.vars = c("sector", "source_org", "destination_org"), 
            .f = trimws)
  

donor_cluster %>% 
  filter(boundary == "Incoming") %>% 
  pivot_longer(cols = c(source_org, destination_org), 
               names_to = "type", 
               values_to = "org_name") %>% 
  pairwise_count(org_name, flow_id, sort = TRUE, upper = FALSE, wt = amount_us) %>% 
  rename(org1 = item1, 
         org2 = item2, 
         USD = n) %>% 
  mutate(`%` = round(USD / sum(USD, na.rm = TRUE) * 100, digits = 2)) %>% 
  head(20) %>% 
  flextable() %>% 
  set_caption(caption = "Top donor and cluster pairs") %>% 
  theme_zebra() %>% 
  add_footer_lines("as of 18 January 2023")


```

```{r}
donor_cluster %>% 
  
  
  lm(amount_us ~ name) %>% 
  summary() %>% 
  tidy() 
  

donor_cluster %>% 
  filter(!is.na(amount_us)) %>% 
  nest(-name) %>% 
  mutate(model = map(data ~ t.test(.$amount_us))) %>% 
  unnest_legacy(map(model, tidy))


donor_cluster %>%
  filter(boundary == "Incoming") %>% 
  mutate(sector = trimws(sector)) %>%
  filter(!is.na(amount_us)) %>% 
  lm(amount_us ~ sector, data = .) %>% 
  tidy(conf.int = TRUE) %>% 
  filter(term != "(Intercept)") %>% 
  mutate(term = str_remove(term, "^sector"), 
         term = str_sub(term, 1, 30), 
         term = fct_reorder(term, estimate)) %>% 
  slice_max(estimate, n = 20) %>% 
  ggplot(aes(x = estimate, 
             y = term, 
             colour = estimate)) + 
  geom_vline(xintercept = 0, lty = 2, colour = "red") + 
  geom_point(size = 2.5) + 
  # geom_errorbarh(aes(xmin = conf.low, 
  #                    xmax = conf.high), 
  #                height = 0.2) +
  scale_colour_viridis(option = "turbo") + 
  scale_x_continuous(labels = number_format(suffix = "M",
                                            prefix = "$ ", 
                                            scale = 1/1000000), 
                     trans = "log10") +
  theme(legend.position = "none") + 
  labs(x = "Estimated funding received per donor", y = "", 
       title = "Coefficient plot, USD funding per flow") + 

donor_cluster %>%
  filter(boundary == "Incoming") %>% 
  mutate(sector = trimws(source_org)) %>%
  filter(!is.na(amount_us)) %>% 
  lm(amount_us ~ source_org, data = .) %>% 
  tidy(conf.int = TRUE) %>% 
  filter(term != "(Intercept)") %>% 
  mutate(term = str_remove(term, "^source_org"), 
         term = str_sub(term, 1, 30), 
         term = fct_reorder(term, estimate)) %>% 
  slice_max(estimate, n = 20) %>% 
  ggplot(aes(x = estimate, 
             y = term, 
             colour = estimate)) + 
    geom_vline(xintercept = 0, lty = 2, colour = "red") + 
    geom_point(size = 2.5) + 
  # geom_errorbarh(aes(xmin = conf.low, 
  #                    xmax = conf.high), 
  #                height = 0.2) +
  scale_colour_viridis(option = "turbo") + 
  scale_x_continuous(labels = number_format(suffix = "M",
                                            prefix = "$ ", 
                                            scale = 1/1000000), 
                     trans = "log10") +
  theme(legend.position = "none") + 
  labs(x = "Estimated funding per project", y = "", 
       title = "Coefficient plot, USD funding per flow") +

donor_cluster %>%
  filter(boundary == "Incoming") %>% 
  mutate(sector = trimws(destination_org)) %>%
  filter(!is.na(amount_us)) %>% 
  lm(amount_us ~ destination_org, data = .) %>% 
  tidy(conf.int = TRUE) %>% 
  filter(term != "(Intercept)") %>% 
  mutate(term = str_remove(term, "^destination_org"), 
         term = str_sub(term, 1, 30), 
         term = fct_reorder(term, estimate)) %>% 
  slice_max(estimate, n = 20) %>%
  ggplot(aes(x = estimate, 
             y = term, 
             colour = estimate)) +
  geom_vline(xintercept = 0, lty = 2, colour = "red") + 
  geom_point(size = 2.5) + 
  # geom_errorbarh(aes(xmin = conf.low, 
  #                    xmax = conf.high), 
  #                height = 0.2) +
  scale_colour_viridis(option = "turbo") + 
  scale_x_continuous(labels = number_format(suffix = "M",
                                            prefix = "$ ", 
                                            scale = 1/1000000), 
                     trans = "log10") +
  theme(legend.position = "none") + 
  labs(x = "Estimated funding per project", y = "", 
       title = "Coefficient plot, USD funding per flow") + 
  
  plot_layout(ncol = 1)

```

I don't know why this is not working 

```{r}
plot_money <- function(tbl, var_name) {
  
  var_name <- enquo(var_name)
  var_namen <- quo_name(var_name)
  
  tbl %>% 
    filter(boundary == "Incoming") %>%  
    filter(!is.na(amount_us)) %>% 
    lm(amount_us ~ !!var_name, data = .) %>% 
    tidy(conf.int = TRUE) %>% 
    filter(term != "(Intercept)") %>% 
    mutate(term = str_remove(term, paste0(var_namen)), 
           term = fct_reorder(term, estimate)) %>% 
    slice_max(abs(estimate), n = 20) %>% 
    ggplot(aes(x = estimate, 
               y = term, 
               colour = estimate)) + 
    geom_vline(xintercept = 0, lty = 2, colour = "red") + 
    geom_point(size = 2.5) + 
    geom_errorbarh(aes(xmin = conf.low, 
                       xmax = conf.high), 
                   height = 0.2) + 
    scale_colour_viridis(option = "turbo") + 
    scale_x_continuous(labels = number_format(suffix = "M", 
                                              prefix = "$ ", 
                                              scale = 1/1000000)) + 
    theme(legend.position = "none") +
    labs(x = "Estimate", 
         y = "")
    
}

plot_money(donor_cluster, "sector")
```




