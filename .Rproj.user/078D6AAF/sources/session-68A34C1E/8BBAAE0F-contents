---
title: "Untitled"
output: html_document
date: '2023-01-06'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.width=9, message = FALSE, warning=FALSE)

library(tidyverse)
library(tidytext)
library(tidymodels)
library(broom)
library(ggraph)
library(tidylo)
library(widyr)
library(janitor)
library(lubridate)
library(SnowballC)
library(magrittr)
library(patchwork)
library(tidylo)
library(DT)

`%out%` <- Negate(`%in%`)

range_wna <- function(x){(x-min(x, na.rm = TRUE))/(max(x, na.rm = TRUE)-min(x, na.rm = TRUE))}

theme_set(theme_light())

```


## Data


```{r}

ukr_links <- read_csv("./data/reliefweb_ukr_links.csv") %>% 
  rename(url = value) %>% 
  mutate(id = row_number())

# This is the list of Spanish and French links 
to_remove <- read_csv("./data/links_to_remove.csv") %>% 
  rename(url = value) %>% 
  pull(url)

# This is not the right way to handle the sectors
# You need to use separate_rows and just have a separate dataset called rw_sectors
rw <- read_csv("./data/scraped_full_20230101_20220101.csv") %>% 
  mutate(date = dmy(date), 
         month = month(date)) %>% 
  left_join(ukr_links, by = c("link" = "url")) %>%
  filter(link %out% to_remove) %>% 
  mutate(theme = str_replace_all(theme, "\n", ",")) %>% 
  mutate(agriculture = ifelse(str_detect(theme, "Agriculture"), 1, 0), 
         cccm = ifelse(str_detect(theme, "Camp Coordination and Camp Management"), 1, 0),
         cca = ifelse(str_detect(theme, "Climate Change and Environment"), 1, 0),
         contributions = ifelse(str_detect(theme, "Contributions"), 1, 0),
         coordination = ifelse(str_detect(theme, "Coordination"), 1, 0),
         disaster_management = ifelse(str_detect(theme, "Disaster Management"), 1, 0),
         education = ifelse(str_detect(theme, "Education"), 1, 0),
         food_nutrition = ifelse(str_detect(theme, "Food and Nutrition"), 1, 0),
         gender = ifelse(str_detect(theme, "Gender"), 1, 0),
         health = ifelse(str_detect(theme, "Health"), 1, 0),
         hiv_aids = ifelse(str_detect(theme, "HIV/Aids"), 1, 0),
         financing = ifelse(str_detect(theme, "Humanitarian Financing"), 1, 0),
         logs_telecoms = ifelse(str_detect(theme, "Logistics and Telecommunications"), 1, 0),
         mine_action = ifelse(str_detect(theme, "Mine Action"), 1, 0),
         peacebuiling = ifelse(str_detect(theme, "Peacekeeping and Peacebuilding"), 1, 0),
         protection_human_rights = ifelse(str_detect(theme, "Protection and Human Rights"), 1, 0),
         recovery = ifelse(str_detect(theme, "Recovery and Reconstruction"), 1, 0), 
         security = ifelse(str_detect(theme, "Safety and Security"), 1, 0),
         shelter_nfi = ifelse(str_detect(theme, "Shelter and Non-Food Items"), 1, 0),
         wash = ifelse(str_detect(theme, "Water Sanitation Hygiene"), 1, 0))  
 
sectors <- c("Agriculture",
             "Camp Coordination and Camp Management",
             "Climate Change and Environment",
             "Contributions",
             "Coordination",
             "Disaster Management",
             "Education",
             "Food and Nutrition",
             "Gender",
             "Health",
             "HIV/Aids",
             "Humanitarian Financing",
             "Logistics and Telecommunications",
             "Mine Action",
             "Peacekeeping and Peacebuilding",
             "Protection and Human Rights",
             "Recovery and Reconstruction",
             "Safety and Security",
             "Shelter and Non-Food Items",
             "Water Sanitation Hygiene")
# I don't think this is necessary anymore, I'm just going to treat this like 
# genres in an IMDB scrape 
# rw_dup <- rw %>% 
#   separate_rows(theme, 
#            sep = "\n")



```

```{r}
top_40 <- rw %>% 
  count(source, sort = TRUE) %>% 
  filter(n >= 25) %>% 
  pull(source)

```


```{r}
rw %>% 
  count(type, sort = TRUE) %>% 
  ggplot(aes(x = n, 
             y = fct_reorder(type, n))) + 
  geom_col(aes(fill = type), 
           show.legend = FALSE) + 
  geom_text(aes(label = n),
            size = 3, 
            hjust = "inward")
```


## Titles 

```{r}
titles <- rw %>% 
  filter(source %out% c("ACLED", "OSCE", "IAEA")) %>% 
  select(id, title) %>% 
  unnest_tokens(word, title) %>% 
  count(id, word, sort = TRUE) %>% 
  anti_join(stop_words, by = "word") %>% 
  mutate(word = str_remove_all(word, "'")) %>% 
  mutate(stem = wordStem(word, language = "porter")) %>% 
  filter(stem %out% c("de", "des", "la", "las", "le", "les", 
                      "en", "el", "http", "se", "ses")) %>% 
  mutate(stem = recode(stem, 
                       "ukraine’" = "ukrain")) %>% 
  filter(nchar(stem) < 20)
  
```


```{r}
set.seed(2023)

titles %>% 
  pairwise_count(word, id, sort = TRUE, upper = FALSE) %>%
  filter(str_detect(item1, "[a-z]") & str_detect(item2, "[a-z]")) %>%
  filter(n >= 30) %>%
  igraph::graph_from_data_frame() %>% 
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "cyan4") +
  geom_node_point(size = 2) +
  geom_node_text(aes(label = name), repel = TRUE, size = 2, 
                 point.padding = unit(0.2, "lines")) + 
  labs(title = "Word pairs in Reliefweb Ukraine articles",
       subtitle = "2022-01-01 to 2023-01-01")

ggsave("./plots/title_network_graph.png", width = 42, height = 29.7, 
       units = "cm", dpi = 400)
```


## Words 

You wrote this all beetter in th e
Reliefweb, in general, is full of fairly repetitive content.
Text mining algorithms have uncovered a substantial amount of boilerplate. 
This is why we examine the term frequency (the number of times a word appears in the corpus) and the term frequency-inverse document frequency (tf-idf), which balances the most common words against the words tha

I think you should remove OSCE, ACLED and IAEA from the corpus, since these have been included in the ACLED database anyway 
Furthermore, if one examines the ACLED entries, one notes that 

```{r}

words_source <- rw %>% 
  select(id, source, body) %>% 
  filter(source %out% c("ACLED", "OSCE", "IAEA")) %>% 
  unnest_tokens(word, body) %>% 
  count(source, word, sort = TRUE) %>% 
  anti_join(stop_words, by = "word") %>% 
  filter(str_detect(word, "[a-z]")) %>%
  mutate(word = str_remove_all(word, "'")) %>% 
  mutate(stem = wordStem(word, language = "porter")) %>% 
  filter(stem %out% c("de", "des", "la", "las", "le", "les", 
                      "en", "el", "http", "se", "ses")) %>% 
  # filter(!str_detect(word, "ukraine|ukrainian|people|humanitarian|support|including|country|\\.")) %>% 
  # mutate(stem = recode(stem, 
  #                      "ukraine’" = "ukrain")) %>% 
  filter(nchar(stem) < 20)

words_id <- rw %>% 
  select(id, source, body) %>% 
  filter(source %out% c("ACLED", "OSCE", "IAEA")) %>% 
  unnest_tokens(word, body) %>% 
  count(id, word, sort = TRUE) %>% 
  anti_join(stop_words, by = "word") %>% 
  filter(str_detect(word, "[a-z]")) %>%
  mutate(word = str_remove_all(word, "'")) %>% 
  mutate(stem = wordStem(word, language = "porter")) %>% 
  filter(stem %out% c("de", "des", "la", "las", "le", "les", 
                      "en", "el", "http", "se", "ses")) %>% 
  # filter(!str_detect(word, "ukraine|ukrainian|people|humanitarian|support|including|country|\\.")) %>% 
  # mutate(stem = recode(stem, 
  #                      "ukraine’" = "ukrain")) %>% 
  filter(nchar(stem) < 20)


```


### Word pair network graph

However, what we can see is that all these most common words are largely boilerplate. Though "systems" and "supplies" might lead 
children, humanitarian assistance, displaced
Really, what else was gonna be said 

```{r}
set.seed(2023)

words_to_remove <- c("ukraine", "ukranian", "ukrainian", "humanitarian", "support", "people", 
                     "including", "international", "countries", "country")

# Be aware that word_pairs as it is will return a df of 111 million rows 
# Consider setting a better threshold
word_pairs <- words %>%
  filter(word %out% words_to_remove) %>% 
  pairwise_count(word, source, sort = TRUE, upper = FALSE) %>% 
  filter(str_detect(item1, "[a-z]") & str_detect(item2, "[a-z]")) %>% 
  filter(n > 100)


word_pair_network_graph <- words_id %>%
  # filter(word %out% words_to_remove) %>% 
  pairwise_count(word, id, sort = TRUE, upper = FALSE) %>%
  filter(str_detect(item1, "[a-z]") & str_detect(item2, "[a-z]")) %>%
  # Start at 500, remember how many pairs ther are 
  filter(n > 500) %>% 
  igraph::graph_from_data_frame() %>% 
  ggraph(layout = "stress") +
  geom_edge_link(aes(edge_alpha = n/2, edge_width = n), edge_colour = "tomato") +
  geom_node_point(size = 2) +
  geom_node_text(aes(label = name), repel = TRUE, size = 3, 
                 point.padding = unit(0.2, "lines")) +
  theme_void() + 
  labs(title = "Most common word pairs in Reliefweb articles on Ukraine", 
       subtitle = "2022-01-01 to 2023-01-01; lines show number of co-occurences", 
       edge_width = "") + 
  # Removing the legends for edge width and edge alpha 
  guides(edge_alpha = "none", 
         edge_width = "none") + 
  theme(plot.background = element_rect(fill = "white"))
  

ggsave(word_pair_network_graph, 
       filename = "./plots/word_pair_network_graph.png",
       dpi = 400, width = 16.5, height = 11.7, units = "in") 

word_pairs %>% 
  filter(n >= 800) %>% 
  arrange(desc(n))


```


### Tf-idf words

Unfortunately, the tf-idf for words is not exactly that useful 
The interpretabilty at the bigram level is just much better 


```{r}
tf_idf_words <- words_id %>% 
  bind_tf_idf(stem, id, n) %>% 
  bind_log_odds(stem, id, n)



tf_idf %>%
  group_by(source) %>% 
  filter(source %in% c("ICRC", "OCHA", "UNHCR", "UNICEF", 
                       "OHCHR", "ECHO")) %>% 
  slice_max(tf_idf, n = 20) %>% 
  ungroup() %>% 
  ggplot(aes(x = tf_idf, 
             y = reorder_within(stem, tf_idf, source, fun = sum), 
             fill = source))+ 
  geom_col(show.legend = FALSE) + 
  facet_wrap(~ source, scales = "free") + 
  scale_y_reordered() + 
  labs(x = "Term frequency", y = "")
  
tf_idf %>% 
  filter(source == "World Vision") %>% 
  count(stem, sort = TRUE)
  
ukr_words %>% 
  filter(stem == "hiroshima")


```

### Log odds

```{r}
log_odds <- bigrams %>% 
  bind_log_odds(bigram, source, n) %>% 
  filter(!is.infinite(log_odds_weighted))

log_odds_source <- function(tbl, source) {
  
  source <- enquo(source)
  sourcen <- quo_name(source)
  
  tbl %>%
    filter(source == !!source) %>%
    arrange(desc(log_odds_weighted)) %>%
    head(15) %>%
    ggplot(aes(x = log_odds_weighted,
               y = fct_reorder(word, log_odds_weighted))) +
    geom_col(show.legend = FALSE,
             fill = "cornflowerblue") +
    labs(x = "Log odds",y = "",
      title = paste0("Term frequency -- ", sourcen),
      subtitle = "2022-01-01 to 2023-01-01")
}


log_odds %>% 
  filter(source %in% c("USAID", "EC", "ECHO")) %>% 
  group_by(source) %>% 
  slice_max(log_odds_weighted, n = 15) %>% 
  ungroup() %>% 
  ggplot(aes(x = log_odds_weighted,
               y = fct_reorder(word, log_odds_weighted))) +
  geom_col(show.legend = FALSE,
           aes(fill = source)) +
  facet_wrap(~ source, scales = "free") + 
  labs(x = "Log odds",y = "",
       title = "Log odds -- selected actors",
       subtitle = "2022-01-01 to 2023-01-01")

log_odds %>% 
  filter(source == "WFP") %>% 
  arrange(desc(log_odds_weighted))
  

```



```{r}

frequency_by_source <- function(tbl, source) {
  
  source <- enquo(source)
  sourcen <- quo_name(source)
  
  tbl %>% 
    filter(source == !!source) %>%
    arrange(desc(tf)) %>% 
    head(20) %>%
    ggplot(aes(x = tf, 
               y = fct_reorder(word, tf, .fun = sum)))+
    geom_col(show.legend = FALSE, 
             fill = "cornflowerblue") + 
    labs(x = "Term frequency", y = "", 
         title = paste0("Term frequency -- ", sourcen), 
         subtitle = "2022-01-01 to 2023-01-01") + 

  tbl %>%
    filter(source == !!source) %>% 
    arrange(desc(tf_idf)) %>% 
    head(20) %>%
    ggplot(aes(x = tf_idf, 
               y = fct_reorder(word, tf_idf, .fun = sum)))+
    geom_col(show.legend = FALSE, 
             fill = "tomato") + 
    labs(x = "Tf-idf", y = "", 
         title = paste0("Tf-idf -- ", sourcen), 
         subtitle = "2022-01-01 to 2023-01-01") + 
    
  tbl %>%
    filter(source == !!source) %>%
    filter(!is.infinite(log_odds_weighted)) %>% 
    arrange(desc(log_odds_weighted)) %>%
    head(15) %>%
    ggplot(aes(x = log_odds_weighted,
               y = fct_reorder(word, log_odds_weighted))) +
    geom_col(show.legend = FALSE,
             fill = "forestgreen") +
    labs(x = "Log odds",y = "",
      title = paste0("Term frequency -- ", sourcen),
      subtitle = "2022-01-01 to 2023-01-01")
    
}

frequency_by_source(tf_idf, "UNDP")


```

```{r}
rw %>% 
  count(source, sort = TRUE)
```


I guess IMC really are CIA 

```{r}

# Article checking function

rw %>% 
  # Including tolower has significantly impacted performance
  filter(source == "UNFPA" & str_detect(body, "Action Plan")) %>%
  sample_n(1) %>% 
  pull(body, title)

rw %>% 
  filter(id == 
           tf_idf %>%
           filter(source == "ACAPS" & word == "slipping") %>%
           sample_n(1) %>%
           pull(id)) %>%
  pull(body)

ukr_links %>% filter(id == 2094) %>% pull(url)
```




## Bigrams


```{r}
bigrams <- rw %>% 
  # You're filtering out one clump here 
  # filter(source %out% c("IAEA", "ACLED")) %>% 
  select(source, id, body) %>% 
  unnest_tokens(bigram, body, token = "ngrams", n = 2) %>% 
  filter(!is.na(bigram)) %>% 
  separate(bigram, c("word1", "word2"), sep = " ") %>% 
  filter(!word1 %in% stop_words$word & (nchar(word1) < 20)) %>% 
  filter(!word2 %in% stop_words$word & (nchar(word2) < 20)) %>% 
  filter(str_detect(word1, "[a-z]") & !str_detect(word1, "<")) %>% 
  filter(str_detect(word2, "[a-z]") & !str_detect(word2, "<")) %>% 
  unite(bigram, word1, word2, sep = " ")
  
# bigram_counts <- rw %>% 
#   # You're filtering out one clump here 
#   # filter(source %out% c("IAEA", "ACLED")) %>% 
#   select(source, id, body) %>% 
#   unnest_tokens(bigram, body, token = "ngrams", n = 2) %>% 
#   filter(!is.na(bigram)) %>% 
#   separate(bigram, c("word1", "word2"), sep = " ") %>% 
#   filter(!word1 %in% stop_words$word & (nchar(word1) < 20)) %>% 
#   filter(!word2 %in% stop_words$word & (nchar(word2) < 20)) %>% 
#   filter(str_detect(word1, "[a-z]") & !str_detect(word1, "<")) %>% 
#   filter(str_detect(word2, "[a-z]") & !str_detect(word2, "<")) %>% 
#   count(word1, # word2, sort = TRUE)

tf_idf_bigrams <- bigrams %>%
  count(id, bigram) %>% 
  bind_tf_idf(bigram, id, n) %>% 
  bind_log_odds(bigram, id, n) %>% 
  arrange(desc(tf_idf)) %>% 
  left_join(rw %>% 
              select(id, source), 
            by = "id") %>% 
  filter(bigram %out% c("million people", "humanitarian assistance")) %>% 
  rename(word = bigram) %>% 
  filter(n > 2)
  

```


```{r}
bigrams_month <- rw %>% 
  filter(source %out% c("IAEA", "ACLED", "OSCE")) %>% 
  select(id, month, body) %>% 
  unnest_tokens(bigram, body, token = "ngrams", n = 2) %>% 
  filter(!is.na(bigram)) %>% 
  separate(bigram, c("word1", "word2"), sep = " ") %>% 
  filter(!word1 %in% stop_words$word & (nchar(word1) < 20)) %>% 
  filter(!word2 %in% stop_words$word & (nchar(word2) < 20)) %>% 
  filter(str_detect(word1, "[a-z]") & !str_detect(word1, "<")) %>% 
  filter(str_detect(word2, "[a-z]") & !str_detect(word2, "<")) %>% 
  unite(bigram, word1, word2, sep = " ")

tf_idf_bigram_months <- bigrams_month %>% 
  count(month, bigram) %>% 
  bind_tf_idf(bigram, month, n) %>% 
  bind_log_odds(bigram, month, n) %>% 
  arrange(desc(tf_idf)) %>% 
  filter(bigram %out% c("million people", "humanitarian assistance")) %>% 
  rename(word = bigram) %>% 
  filter(n > 1)

tf_idf_bigrams %>% 
  ggplot(aes(x = n)) + 
  geom_histogram() + 
  scale_x_log10() + 
  scale_y_log10() + 
  geom_vline(xintercept = 2, lty = 2, colour = "red") + 
  geom_vline(xintercept = 3, lty = 2, colour = "orange")
  
```

### Tf-idf months 

This is actually fairly interpretable. 

```{r}

tf_idf_bigram_months %>%
  filter(!is.infinite(log_odds_weighted)) %>%
  mutate(month = factor(month.abb[month],levels = month.abb)) %>%
  group_by(month) %>% 
  arrange(desc(log_odds_weighted), desc(tf_idf)) %>% 
  mutate(rank = row_number()) %>% 
  filter(rank <= 20) %>% 
  ungroup() %>% 
  ggplot(aes(x = log_odds_weighted, 
             y = fct_reorder(word, log_odds_weighted), 
             fill = month)) + 
  geom_col(show.legend = FALSE) +
  scale_fill_viridis_d() + 
  facet_wrap(~ month, scales = "free") + 
  labs(x = "Log-odds", 
       y = "", 
       title = "Log-odds of word specificity to month") +
   theme(legend.position = "none",
        axis.text.x = element_text(size = 5, hjust = 1, vjust = 1), 
        axis.text.y = element_text(size = 7), 
        strip.text = element_text(size = 8, face = "bold"),
        strip.background = element_rect(fill = "#212121")) 
```

```{r}
rw %>% 
  filter(month == 6 & str_detect(body, "mental toll") ) %>% 
  sample_n(1) %>% 
  pull(body, link)

# Stronger checking function, which will always work, unlike the one above
rw %>% 
  filter(id == 
           bigrams_month %>%
           filter(month == 8 & bigram  == "operationally feasible") %>%
           sample_n(1) %>%
           pull(id)) %>%
  # Is this a fluke? How am I able to pull two columns?
  pull(body, source) 

```



### Tf-idf bigrams

Interpretability is very high on the bigram dataset, however, only the broadest strokes are noticed. One clear example is OCHA's result. 

Term frequency here shows us the phrases that each organisation uses most commonly. Often, their name or their mandate ("food assistance") will appear in this list. 

Tf-idf (term frequency-inverse document frequency) is another measure by which significant words are identified. The term frequency -- the number of times a term appears in the corpus -- is tempered by the inverse document frequency, which discounts words that tend to appear again and again (think "humanitarian assistance"). The combined metric is useful for determining which words are common, but not too common. A suspicious-minded person might even say that this is where we may find what agencies really care about, as opposed to the boilerplate that they include in every report. 

Finally, we also evaluate the corpuses by looking at their log odds. This highlights words that are more likely to originate from a source over any other sources. This is where we may see the unique information that each source brings to the table.  


### Save source bigram plots 

```{r}
save_bigrams <- function(df, filename) {
  
  bigram_patchwork <- df %>% 
    arrange(desc(tf)) %>% 
    head(20) %>% 
    ggplot(aes(x = tf, 
               y = fct_reorder(word, tf, .fun = sum)))+
    geom_col(show.legend = FALSE, 
             fill = "cornflowerblue") + 
    labs(x = "Term frequency", y = "", 
         title = paste0("Term frequency: ", filename), 
         subtitle = "2022-01-01 to 2023-01-01") + 

  df %>%
    arrange(desc(tf_idf)) %>% 
    head(20) %>%
    ggplot(aes(x = tf_idf, 
               y = fct_reorder(word, tf_idf, .fun = sum)))+
    geom_col(show.legend = FALSE, 
             fill = "tomato") + 
    labs(x = "Tf-idf", y = "", 
         title = paste0("Tf-idf: ", filename), 
         subtitle = "2022-01-01 to 2023-01-01") +
  
  df %>% 
    filter(!is.infinite(log_odds_weighted)) %>% 
    arrange(desc(log_odds_weighted)) %>% 
    head(20) %>%
    ggplot(aes(x = log_odds_weighted, 
               y = fct_reorder(word, log_odds_weighted, .fun = sum)))+
    geom_col(show.legend = FALSE, 
             fill = "forestgreen") + 
    labs(x = "Log odds", y = "", 
         title = paste0("Log odds: ", filename), 
         subtitle = "2022-01-01 to 2023-01-01")
  
  ggsave(plot = bigram_patchwork,
         filename = paste0("./plots/bigram_source/",filename, ".png"),
         device = "png",
         dpi = 300, height = 11, width = 14, units = "in")
  
}
```

#### Remember
So, I think, for the best results, you should separate the sources into two categories, one with many 

```{r}
more_than_5 <- rw %>% 
  count(source, sort = TRUE) %>% 
  filter(n >= 5) %>% 
  pull(source)
```


```{r}
# Throws an error, but still works 
# This error occurs when you ask R to overwrite an existing file 
tf_idf_bigrams %>% 
  filter(source %in% more_than_5) %>% 
  nest(-source) %$%
  walk2(data, source, save_bigrams)
```

```{r}

# Article checking function
rw %>% 
filter(source == "WFP" & str_detect(body, "agency convoys")) %>%
  sample_n(1) %>% 
  pull(body, id)

rw %>% 
  filter(id == 
           tf_idf %>%
           filter(word == "gou") %>%
           sample_n(1) %>%
           pull(id)) %>%
  # Is this a fluke? How am I able to pull two columns?
  pull(body, source)

ukr_links %>% filter(id == 2118) %>% pull(url)


# alternative that uses id
# this is slower (by 10ms) and uses a bit more memory, 
# assuming you understand anything at all about profvis 
rw %>% 
  filter(id == 
           tf_idf_bigrams %>%
           filter(source == "WFP" & word == "agency convoys") %>%
           sample_n(1) %>%
           pull(id)) %>%
  pull(body, date)

```

What is the game the EC is playing with the French presidency? Macron is a backstabbing fool who already tried to go around the Americans to get a deal with the Russians. My assumption here is that the EC is humouring French attempts to return to Great Power status. 

```{r}
tf_idf_bigrams %>% 
  filter(bigram %out% c("million people", "humanitarian assistance")) %>% 
  rename(word = bigram) %>% 
bigram_frequency_by_source("EC")
```

Ukraine's only avenue for agricultural and non-agricultural exports is the EU? 

We can also see the World Bank being mostly inactive. There are no mentions of credit or financing -- without that, their influence is diminished. They do mention the phrase "future assessments". Strangely also, they have publicised the 

We can also almost definitely ignore the World Bank for the moment as well. I see no mentions of credit or financing. With the phrase "future assessments" coming at the list of log odds. I'd be interested in seeing what their relationship is with the EIB. 

I wouldn't use reliefweb articles to parse the thinking of the Ukrainian government. 

```{r}
tf_idf_bigrams %>% 
  filter(bigram %out% c("million people", "humanitarian assistance")) %>% 
  rename(word = bigram) %>% 
bigram_frequency_by_source("World Vision")
```

We include World Vision because they have the clearest indication, from any NGO, of a multi-sector programme. Developing a list of 


## Network graphs

### Bigrams

```{r}
set.seed(2023)

network_graph <- bigrams %>% 
  filter(bigram %out% c("million people", 
                        #"internally displaced", 
                        "united nations", 
                        #"human rights", 
                        #"humanitarian assistance",
                        "february 2022")) %>% 
  distinct(id, bigram) %>% 
  add_count(bigram) %>% 
  filter(n >= 50) %>% 
  pairwise_cor(bigram, id, sort = TRUE) %>% 
  filter(correlation >= .15) %>% 
  igraph::graph_from_data_frame() %>% 
  ggraph(layout = "kk") + 
  geom_edge_fan(aes(alpha = correlation),
                colour = "cornflowerblue",
                check_overlap = TRUE,
                width = .1) + 
  scale_alpha_continuous(range = c(.1, .4)) + 
  geom_node_point(colour = "black", alpha = .1, size = .5) + 
  geom_node_text(aes(label = name), size = 1, 
                 vjust = 1, hjust = 1, 
                 check_overlap = TRUE) +
  theme(legend.position = "none") + 
  labs(title = "Network graphs of Reliefweb bigrams related to the Ukraine conflict in 2022", 
       subtitle = "Data source: https://reliefweb.int/updates?advanced-search=%28PC241%29_%28DA20220101-20230104%29")

ggsave("./plots/network_graph_kk.png", network_graph, width = 42, height = 29.7, units = "cm", dpi = 300)
  
```

### Words

There really is quite limited utility in a network graph based on tokens unnested by source. 

```{r}
set.seed(2023)

network_graph <- words_id %>% 
  filter(word %out% words_to_remove) %>% 
  distinct(id, word) %>% 
  add_count(word) %>% 
  filter(n >= 200) %>% 
  pairwise_cor(word, id, sort = TRUE) %>% 
  filter(correlation >= .15) %>% 
  igraph::graph_from_data_frame() %>% 
  ggraph(layout = "fr") + 
  geom_edge_link(aes(edge_alpha = correlation, 
                     edge_width = correlation), 
                 edge_colour = "tomato") + 
  geom_node_point(size = 2) + 
  geom_node_text(aes(label = name), 
                 repel = TRUE, 
                 size = 2, 
                 point.padding = unit(0.2, "lines")) + 
  theme(legend.position = "none") + 
  labs(title = "Network graphs of Reliefweb bigrams related to the Ukraine conflict in 2022", 
       subtitle = "Data source: https://reliefweb.int/updates?advanced-search=%28PC241%29_%28DA20220101-20230104%29")

geom_edge_link(aes(edge_alpha = n/2, edge_width = n), edge_colour = "tomato") +
  geom_node_point(size = 2) +
  geom_node_text(aes(label = name), repel = TRUE, size = 3, 
                 point.padding = unit(0.2, "lines")) +
  theme_void() + 
  labs(title = "Most common word pairs in Reliefweb articles on Ukraine", 
       subtitle = "2022-01-01 to 2023-01-01; lines show number of co-occurences", 
       edge_width = "") + 
  # Removing the legends for edge width and edge alpha 
  guides(edge_alpha = "none", 
         edge_width = "none") + 
  theme(plot.background = element_rect(fill = "white"))


ggsave("./plots/network_graph_fr.png", network_graph, width = 42, height = 29.7, units = "cm", dpi = 400)

  
```

## Pairwise correlations 

```{r dt-pairwise-words}
words_id %>%  
  filter(word %out% words_to_remove) %>% 
  distinct(id, word) %>% 
  add_count(word) %>% 
  filter(n >= 120) %>% 
  pairwise_cor(word, id, sort = TRUE) %>%
  rename(word = item1, match = item2) %>% 
  group_by(word) %>% 
  top_n(correlation, n = 20) %>% 
  datatable(filter = list(position = "top", clear = FALSE), 
            options = list(pageLength = 10, scrollX = TRUE,
                           search = list(regex = TRUE)
  #                         
  #                         ,
  #                                         initComplete = htmlwidgets::JS(
  #        "function(settings, json) {",
  #        paste0("$(this.api().table().container()).css({'font-size': '", "8.5pt", "'});"),
  #        "}")
       ),
  caption = htmltools::tags$caption(style = 'caption-side: top; 
                                    text-align: center; 
                                    color:black; font-size:140% ;',
                                    "Pairwise correlations in Reliefweb Ukraine articles")
     ) %>% 
  formatRound(c("correlation"), digits = 3)

```

```{r dt-check-bigrams}

ukr_links %>% 
  filter(id == rw %>% 
           filter(str_detect(body, "Izium") & 
                    str_detect(body, "boys")) %>% 
           sample_n(1) %>% 
           pull(id)) %>%
  pull(url)
```


