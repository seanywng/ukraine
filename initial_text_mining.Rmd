---
title: "Untitled"
output: html_document
date: '2023-01-06'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.width=9, message = FALSE, warning=FALSE)

library(tidyverse)
library(tidytext)
library(tidymodels)
library(broom)
library(ggraph)
library(tidylo)
library(widyr)
library(janitor)
library(lubridate)
library(SnowballC)
library(magrittr)
library(patchwork)
library(tidylo)

`%out%` <- Negate(`%in%`)

```


## Data

```{r}

ukr_links <- read_csv("./data/reliefweb_ukr_links.csv") %>% 
  rename(url = value) %>% 
  mutate(id = row_number())

# This is the list of Spanish and French links 
to_remove <- read_csv("./data/links_to_remove.csv") %>% 
  rename(url = value) %>% 
  pull(url)

rw <- read_csv("./data/scraped_full_20230101_20220101.csv") %>% 
  mutate(date = dmy(date)) %>% 
  left_join(ukr_links, by = c("link" = "url")) %>%
  filter(link %out% to_remove) %>% 
  group_by(link) %>% 
  slice(1) %>% 
  ungroup() %>% 
  select(-link) %>% 
  mutate(theme = str_replace_all(theme, "\n", ",")) %>% 
  mutate(agriculture = ifelse(str_detect(theme, "Agriculture"), 1, 0), 
         cccm = ifelse(str_detect(theme, "Camp Coordination and Camp Management"), 1, 0),
         cca = ifelse(str_detect(theme, "Climate Change and Environment"), 1, 0),
         contributions = ifelse(str_detect(theme, "Contributions"), 1, 0),
         coordination = ifelse(str_detect(theme, "Coordination"), 1, 0),
         disaster_management = ifelse(str_detect(theme, "Disaster Management"), 1, 0),
         education = ifelse(str_detect(theme, "Education"), 1, 0),
         food_nutrition = ifelse(str_detect(theme, "Food and Nutrition"), 1, 0),
         gender = ifelse(str_detect(theme, "Gender"), 1, 0),
         health = ifelse(str_detect(theme, "Health"), 1, 0),
         hiv_aids = ifelse(str_detect(theme, "HIV/Aids"), 1, 0),
         financing = ifelse(str_detect(theme, "Humanitarian Financing"), 1, 0),
         logs_telecoms = ifelse(str_detect(theme, "Logistics and Telecommunications"), 1, 0),
         mine_action = ifelse(str_detect(theme, "Mine Action"), 1, 0),
         peacebuiling = ifelse(str_detect(theme, "Peacekeeping and Peacebuilding"), 1, 0),
         protection_human_rights = ifelse(str_detect(theme, "Protection and Human Rights"), 1, 0),
         recovery = ifelse(str_detect(theme, "Recovery and Reconstruction"), 1, 0), 
         security = ifelse(str_detect(theme, "Safety and Security"), 1, 0),
         shelter_nfi = ifelse(str_detect(theme, "Shelter and Non-Food Items"), 1, 0),
         wash = ifelse(str_detect(theme, "Water Sanitation Hygiene"), 1, 0))

# I don't think this is necessary anymore, I'm just going to treat this like 
# genres in an IMDB scrape 
# rw_dup <- rw %>% 
#   separate_rows(theme, 
#            sep = "\n")


```

## Titles 

```{r}
titles <- rw %>% 
  select(id, title) %>% 
  unnest_tokens(word, title) %>% 
  count(id, word, sort = TRUE) %>% 
  anti_join(stop_words, by = "word") %>% 
  mutate(word = str_remove_all(word, "'")) %>% 
  mutate(stem = wordStem(word, language = "porter")) %>% 
  filter(stem %out% c("de", "des", "la", "las", "le", "les", 
                      "en", "el", "http", "se", "ses")) %>% 
  mutate(stem = recode(stem, 
                       "ukraine’" = "ukrain")) %>% 
  filter(nchar(stem) < 20)
  
```


```{r}
set.seed(2023)

titles %>% 
  pairwise_count(word, id, sort = TRUE, upper = FALSE) %>%
  filter(str_detect(item1, "[a-z]") & str_detect(item2, "[a-z]")) %>%
  filter(n >= 50) %>%
  igraph::graph_from_data_frame() %>% 
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "cyan4") +
  geom_node_point(size = 2) +
  geom_node_text(aes(label = name), repel = TRUE, size = 2, 
                 point.padding = unit(0.2, "lines")) +
  theme_void()

```


## Words 

Reliefweb, in general, is full of fairly repetitive content.
Text mining algorithms have uncovered a substantial amount of boilerplate. 
This is why we examine the term frequency (the number of times a word appears in the corpus) and the term frequency-inverse document frequency (tf-idf), which balances the most common words against the words tha

```{r}

words <- rw %>% 
  select(id, body) %>% 
  unnest_tokens(word, body) %>% 
  count(id, word, sort = TRUE) %>% 
  anti_join(stop_words, by = "word") %>% 
  filter(str_detect(word, "[a-z]")) %>%
  mutate(word = str_remove_all(word, "'")) %>% 
  mutate(stem = wordStem(word, language = "porter")) %>% 
  filter(stem %out% c("de", "des", "la", "las", "le", "les", 
                      "en", "el", "http", "se", "ses")) %>% 
  filter(!str_detect(word, "ukraine|ukrainian|people|humanitarian|support|including|country|\\.")) %>% 
  mutate(stem = recode(stem, 
                       "ukraine’" = "ukrain")) %>% 
  filter(nchar(stem) < 20)


```

```{r}
set.seed(2023)

word_pairs <- words %>% 
  pairwise_count(word, id, sort = TRUE, upper = FALSE) %>%
  filter(str_detect(item1, "[a-z]") & str_detect(item2, "[a-z]")) %>% 
  
  filter(n >= 500)

word_pairs %>% 
  igraph::graph_from_data_frame() %>% 
  ggraph(layout = "lgl") +
  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "darkred") +
  geom_node_point(size = 2) +
  geom_node_text(aes(label = name), repel = TRUE, size = 3, 
                 point.padding = unit(0.2, "lines")) +
  theme_void()
  
word_pairs %>% 
  filter(n >= 800) %>% 
  arrange(desc(n))
```


### Tf-idf 

```{r}
tf_idf <- words %>% 
  left_join(rw %>% select(id, source), 
            by = "id") %>% 
  bind_tf_idf(stem, source, n) 

tf_idf %>%
  group_by(source) %>% 
  filter(source %in% c("ICRC", "OCHA", "UNHCR", "UNICEF", 
                       "OHCHR", "ECHO")) %>% 
  slice_max(tf_idf, n = 20) %>% 
  ungroup() %>% 
  ggplot(aes(x = tf_idf, 
             y = reorder_within(stem, tf_idf, source, fun = sum), 
             fill = source))+ 
  geom_col(show.legend = FALSE) + 
  facet_wrap(~ source, scales = "free") + 
  scale_y_reordered() + 
  labs(x = "Term frequency", y = "")
  
tf_idf %>% 
  filter(source == "World Vision") %>% 
  count(stem, sort = TRUE)
  
top_40 <- rw %>% 
  count(source, sort = TRUE) %>% 
  filter(n >= 25) %>% 
  pull(source)

top_10 <- rw %>% 
  count(theme, sort = TRUE) %>% 
  filter(n >= 25) %>% 
  pull(source)

ukr_words %>% 
  filter(stem == "hiroshima")


```

```{r}

frequency_by_source <- function(tbl, source) {
  
  source <- enquo(source)
  sourcen <- quo_name(source)
  
  tbl %>% 
    filter(source == !!source) %>%
    arrange(desc(tf)) %>% 
    head(20) %>%
    ggplot(aes(x = tf, 
               y = fct_reorder(word, tf, .fun = sum)))+
    geom_col(show.legend = FALSE, 
             fill = "cornflowerblue") + 
    labs(x = "Term frequency", y = "", 
         title = paste0("Term frequency -- ", sourcen), 
         subtitle = "2022-01-01 to 2023-01-01") + 

  tbl %>%
    filter(source == !!source) %>% 
    arrange(desc(tf_idf)) %>% 
    head(20) %>%
    ggplot(aes(x = tf_idf, 
               y = fct_reorder(word, tf_idf, .fun = sum)))+
    geom_col(show.legend = FALSE, 
             fill = "tomato") + 
    labs(x = "Tf-idf", y = "", 
         title = paste0("Tf-idf -- ", sourcen), 
         subtitle = "2022-01-01 to 2023-01-01") 
    
}

frequency_by_source(tf_idf, "ACAPS")


```

```{r}
rw %>% 
  count(source, sort = TRUE)
```


I guess IMC really are CIA 

```{r}

# Article checking function

rw %>% 
  filter(id == 
           tf_idf %>%
           filter(source == "ACAPS" & word == "slipping") %>%
           sample_n(1) %>%
           pull(id)) %>%
  pull(body)

```



### Thesis 


## Bigrams


```{r}
bigrams_sep <- rw %>% 
  # You're filtering out one clump here 
  filter(source %out% c("IAEA", "ACLED")) %>% 
  select(id, body) %>% 
  unnest_tokens(bigram, body, token = "ngrams", n = 2) %>% 
  filter(!is.na(bigram)) %>% 
  separate(bigram, c("word1", "word2"), sep = " ") %>% 
  filter(!word1 %in% stop_words$word & (nchar(word1) < 20)) %>% 
  filter(!word2 %in% stop_words$word & (nchar(word2) < 20)) %>% 
  filter(str_detect(word1, "[a-z]") & !str_detect(word1, "<")) %>% 
  filter(str_detect(word2, "[a-z]") & !str_detect(word2, "<"))
  
bigram_counts <- bigrams_sep %>% count(word1, word2, sort = TRUE)
  
bigrams <- bigrams_sep %>%
  unite(bigram, word1, word2, sep = " ") %>% 
  filter(bigram %out% c("million people", 
                        "humanitarian assistance"))
  

tf_idf_bigrams <- bigrams %>%
  count(id, bigram) %>% 
  bind_tf_idf(bigram, id, n) %>% 
  arrange(desc(tf_idf)) %>% 
  left_join(rw %>% 
              select(id, source), 
            by = "id") %>% 
  bind_log_odds(bigram, id, n)
  

bigrams_full <- rw %>% 
  select(id, body) %>% 
  unnest_tokens(bigram, body, token = "ngrams", n = 2) %>% 
  filter(!is.na(bigram)) %>% 
  separate(bigram, c("word1", "word2"), sep = " ") %>% 
  filter(!word1 %in% stop_words$word & (nchar(word1) < 20)) %>% 
  filter(!word2 %in% stop_words$word & (nchar(word2) < 20)) %>% 
  filter(str_detect(word1, "[a-z]") & !str_detect(word1, "<")) %>% 
  filter(str_detect(word2, "[a-z]") & !str_detect(word2, "<")) %>% 
  unite(bigram, word1, word2, sep = " ")

```


### Tf-idf 

Interpretability is very high on the bigram dataset, however, only the broadest strokes are noticed. One clear example is OCHA's result. 

```{r}

bigram_frequency_by_source <- function(tbl, source) {
  
  source <- enquo(source)
  sourcen <- quo_name(source)
  
  tbl %>% 
    filter(source == !!source) %>%
    filter(!is.infinite(log_odds_weighted)) %>% 
    arrange(desc(log_odds_weighted)) %>% 
    head(20) %>%
    ggplot(aes(x = log_odds_weighted, 
               y = fct_reorder(word, log_odds_weighted, .fun = sum)))+
    geom_col(show.legend = FALSE, 
             fill = "cornflowerblue") + 
    labs(x = "Log odds", y = "", 
         title = paste0("Log odds -- ", sourcen), 
         subtitle = "2022-01-01 to 2023-01-01") + 

  tbl %>%
    filter(source == !!source) %>% 
    
    arrange(desc(tf_idf)) %>% 
    head(20) %>%
    ggplot(aes(x = tf_idf, 
               y = fct_reorder(word, tf_idf, .fun = sum)))+
    geom_col(show.legend = FALSE, 
             fill = "tomato") + 
    labs(x = "Tf-idf", y = "", 
         title = paste0("Tf-idf -- ", sourcen), 
         subtitle = "2022-01-01 to 2023-01-01") 
    
}

tf_idf_bigrams %>% 
  rename(word = bigram) %>% 
bigram_frequency_by_source("USAID")


```

```{r}

# Article checking function

rw %>% 
  filter(id == 
           tf_idf_bigrams %>%
           filter(source == "USAID" & bigram == "bakhmut city") %>%
           sample_n(1) %>%
           pull(id)) %>%
  pull(body, date)

rw %>% 
  filter(id == 
           tf_idf %>%
           filter(word == "gou") %>%
           sample_n(1) %>%
           pull(id)) %>%
  # Is this a fluke? How am I able to pull two columns?
  pull(body, source)

```

### Network graph

```{r}
set.seed(2023)

network_graph <- bigrams %>% 
  filter(bigram %out% c("million people", 
                        #"internally displaced", 
                        "united nations", 
                        #"human rights", 
                        #"humanitarian assistance",
                        "february 2022")) %>% 
  distinct(id, bigram) %>% 
  add_count(bigram) %>% 
  filter(n >= 50) %>% 
  pairwise_cor(bigram, id, sort = TRUE) %>% 
  filter(correlation >= .15) %>% 
  igraph::graph_from_data_frame() %>% 
  ggraph(layout = "kk") + 
  geom_edge_fan(aes(alpha = correlation),
                colour = "cornflowerblue",
                check_overlap = TRUE,
                width = .1) + 
  scale_alpha_continuous(range = c(.1, .4)) + 
  geom_node_point(colour = "black", alpha = .1, size = .5) + 
  geom_node_text(aes(label = name), size = 1, 
                 vjust = 1, hjust = 1, 
                 check_overlap = TRUE) +
  theme(legend.position = "none") + 
  labs(title = "Network graphs of Reliefweb bigrams related to the Ukraine conflict in 2022", 
       subtitle = "Data source: https://reliefweb.int/updates?advanced-search=%28PC241%29_%28DA20220101-20230104%29")

ggsave("./plots/network_graph_kk.png", network_graph, width = 42, height = 29.7, units = "cm", dpi = 300)
  
```




